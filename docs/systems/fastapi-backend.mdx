---
sidebar_position: 4
---

# FastAPI Backend Setup and Architecture

## Hero Insight
Transform your AI documentation system into a robust, scalable backend service that handles complex RAG operations with efficiency and reliability.

## Purpose of This Chapter
You will build a comprehensive FastAPI backend architecture that supports RAG operations, content generation, user management, and all the services needed for your AI-powered documentation system.

## Deep Breakdown

### FastAPI Backend Architecture Overview
A production-ready FastAPI backend for AI documentation includes multiple layers:

#### 1. API Layer
- **Route Organization**: Well-structured endpoints for different functionalities
- **Request Validation**: Pydantic models for data validation
- **Response Serialization**: Consistent response formats
- **Error Handling**: Proper HTTP status codes and error messages

#### 2. Service Layer
- **Business Logic**: Core functionality implementations
- **External API Integration**: OpenAI, Qdrant, and other services
- **Caching**: Performance optimization strategies
- **Rate Limiting**: API usage control

#### 3. Data Layer
- **Model Definitions**: Pydantic models for data structures
- **Database Abstractions**: Qdrant client integration
- **Data Validation**: Consistent data integrity
- **Transformation Logic**: Data processing and formatting

#### 4. Utility Layer
- **Configuration Management**: Environment-based settings
- **Logging**: Comprehensive system monitoring
- **Security**: Authentication and authorization
- **Health Checks**: System status monitoring

### Project Structure
```
backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI application instance
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ database.py         # Database connections
‚îÇ   ‚îú‚îÄ‚îÄ models/             # Pydantic models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ request.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ response.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas/            # Pydantic schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ content.py
‚îÇ   ‚îú‚îÄ‚îÄ services/           # Business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embedding_service.py
‚îÇ   ‚îú‚îÄ‚îÄ routes/             # API endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ content.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/              # Utility functions
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ logging.py
‚îÇ       ‚îú‚îÄ‚îÄ validation.py
‚îÇ       ‚îî‚îÄ‚îÄ security.py
‚îú‚îÄ‚îÄ tests/                  # Test files
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .env
```

### Core FastAPI Application Setup

#### Main Application File
**backend/app/main.py:**
```python
from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import logging
import time

from app.config import settings
from app.routes import chat, rag, content
from app.utils.logging import setup_logging
from app.database import init_db

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan events
    """
    # Startup
    setup_logging()
    await init_db()
    logging.info("Application started")
    yield
    # Shutdown
    logging.info("Application shutting down")

app = FastAPI(
    title="AI Documentation Backend",
    description="Backend services for AI-powered documentation system",
    version="1.0.0",
    lifespan=lifespan
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(chat.router, prefix="/api/chat", tags=["chat"])
app.include_router(rag.router, prefix="/api/rag", tags=["rag"])
app.include_router(content.router, prefix="/api/content", tags=["content"])

@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    """
    Add processing time to response headers
    """
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response

@app.get("/")
async def root():
    return {"message": "AI Documentation Backend API", "version": "1.0.0"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": time.time()}

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    """
    Global exception handler
    """
    logging.error(f"Unhandled exception: {str(exc)}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={"detail": "Internal server error"}
    )
```

#### Configuration Management
**backend/app/config.py:**
```python
from pydantic_settings import BaseSettings
from typing import List, Optional

class Settings(BaseSettings):
    # API Configuration
    API_TITLE: str = "AI Documentation Backend"
    API_VERSION: str = "1.0.0"

    # Environment
    ENVIRONMENT: str = "development"
    DEBUG: bool = False

    # OpenAI Configuration
    OPENAI_API_KEY: str
    OPENAI_MODEL: str = "gpt-3.5-turbo"
    EMBEDDING_MODEL: str = "text-embedding-ada-002"

    # Qdrant Configuration
    QDRANT_URL: str
    QDRANT_API_KEY: str
    QDRANT_COLLECTION_NAME: str = "documentation"

    # Database Configuration
    QDRANT_GRPC_ENABLED: bool = True
    QDRANT_TIMEOUT: int = 30

    # Application Settings
    ALLOWED_ORIGINS: List[str] = ["http://localhost:3000", "http://localhost:8080"]
    CORS_ALLOW_CREDENTIALS: bool = True
    CORS_ALLOW_METHODS: List[str] = ["*"]
    CORS_ALLOW_HEADERS: List[str] = ["*"]

    # Performance Settings
    RAG_SEARCH_LIMIT: int = 5
    RAG_SCORE_THRESHOLD: float = 0.3
    MAX_TOKENS: int = 500
    TEMPERATURE: float = 0.7

    # Rate Limiting
    RATE_LIMIT_REQUESTS: int = 100
    RATE_LIMIT_WINDOW: int = 60  # seconds

    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
```

### Data Models and Schemas

#### Request and Response Models
**backend/app/schemas/chat.py:**
```python
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime

class ChatMessage(BaseModel):
    role: str  # "user" or "assistant"
    content: str
    timestamp: Optional[datetime] = None

class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=2000)
    selected_text: Optional[str] = None
    conversation_history: List[ChatMessage] = []
    context_only: bool = False
    temperature: float = Field(default=0.7, ge=0.0, le=1.0)
    max_tokens: int = Field(default=500, ge=100, le=1000)

class ChatResponse(BaseModel):
    response: str
    sources: List[Dict[str, Any]]
    query: str
    conversation_id: Optional[str] = None
    tokens_used: int = 0
    processing_time: float = 0.0

class ChatStreamChunk(BaseModel):
    content: str
    type: str = "chunk"  # "start", "chunk", "end"
    metadata: Optional[Dict[str, Any]] = None
```

**backend/app/schemas/rag.py:**
```python
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional

class SearchRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=1000)
    limit: int = Field(default=5, ge=1, le=20)
    score_threshold: float = Field(default=0.3, ge=0.0, le=1.0)
    filters: Optional[Dict[str, Any]] = None

class SearchResult(BaseModel):
    id: str
    content: str
    metadata: Dict[str, Any]
    score: float

class SearchResponse(BaseModel):
    results: List[SearchResult]
    query: str
    processing_time: float

class RAGRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=1000)
    context_only: bool = False
    selected_text: Optional[str] = None
    max_results: int = Field(default=5, ge=1, le=20)
    temperature: float = Field(default=0.7, ge=0.0, le=1.0)

class RAGResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    query: str
    confidence_score: float
    processing_time: float
```

### Service Layer Implementation

#### RAG Service
**backend/app/services/rag_service.py:**
```python
import logging
from typing import List, Dict, Any, Optional, Tuple
from qdrant_client import QdrantClient
from qdrant_client.http import models
import openai
import tiktoken
from app.config import settings

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        self.qdrant_client = QdrantClient(
            url=settings.QDRANT_URL,
            api_key=settings.QDRANT_API_KEY,
            prefer_grpc=settings.QDRANT_GRPC_ENABLED
        )
        openai.api_key = settings.OPENAI_API_KEY
        self.encoder = tiktoken.encoding_for_model(settings.OPENAI_MODEL)

    async def search_content(
        self,
        query: str,
        limit: int = 5,
        score_threshold: float = 0.3,
        filters: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Search for relevant content in the vector database
        """
        try:
            # Generate embedding for query
            response = openai.Embedding.create(
                input=query,
                model=settings.EMBEDDING_MODEL
            )
            query_embedding = response['data'][0]['embedding']

            # Build search filters
            search_filter = None
            if filters:
                filter_conditions = []
                for key, value in filters.items():
                    filter_conditions.append(
                        models.FieldCondition(
                            key=key,
                            match=models.MatchValue(value=value)
                        )
                    )
                if filter_conditions:
                    search_filter = models.Filter(
                        must=filter_conditions
                    )

            # Perform search in Qdrant
            search_results = self.qdrant_client.search(
                collection_name=settings.QDRANT_COLLECTION_NAME,
                query_vector=query_embedding,
                limit=limit,
                score_threshold=score_threshold,
                with_payload=True,
                query_filter=search_filter
            )

            return [
                {
                    'id': str(result.id),
                    'content': result.payload.get('content', ''),
                    'metadata': result.payload.get('metadata', {}),
                    'score': result.score
                }
                for result in search_results
            ]

        except Exception as e:
            logger.error(f"Error in search_content: {str(e)}")
            raise

    async def generate_response(
        self,
        query: str,
        context_chunks: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, str]]] = None,
        selected_text: Optional[str] = None,
        temperature: float = 0.7
    ) -> Tuple[str, Dict[str, Any]]:
        """
        Generate response using context and conversation history
        """
        try:
            # Build context based on mode
            if selected_text:
                # Mode: Answer only from selected text
                context = f"Answer only based on this text: {selected_text}"
            else:
                # Mode: Use retrieved context
                context = self._assemble_context(context_chunks)

            # Build system message
            system_message = f"""
            You are an AI documentation assistant.
            Use the following context to answer the user's question: {context}

            Provide helpful, accurate responses based on the provided information.
            If the answer cannot be found in the provided context, acknowledge this.
            """

            # Build conversation messages
            messages = [{"role": "system", "content": system_message}]

            if conversation_history:
                messages.extend(conversation_history)

            messages.append({"role": "user", "content": query})

            # Generate response
            response = openai.ChatCompletion.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                temperature=temperature,
                max_tokens=settings.MAX_TOKENS
            )

            generated_text = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else 0

            return generated_text, {
                'tokens_used': tokens_used,
                'model_used': settings.OPENAI_MODEL,
                'confidence': 0.8  # Placeholder for actual confidence calculation
            }

        except Exception as e:
            logger.error(f"Error in generate_response: {str(e)}")
            raise

    def _assemble_context(self, chunks: List[Dict[str, Any]]) -> str:
        """
        Assemble context from retrieved chunks, respecting token limits
        """
        context_parts = []
        total_tokens = 0
        max_tokens = 3000  # Leave room for conversation and response

        for chunk in chunks:
            chunk_text = f"Source: {chunk.get('metadata', {}).get('source', 'Unknown')}\nContent: {chunk['content']}\n\n"
            chunk_tokens = len(self.encoder.encode(chunk_text))

            if total_tokens + chunk_tokens > max_tokens:
                break

            context_parts.append(chunk_text)
            total_tokens += chunk_tokens

        return "".join(context_parts)

    async def process_rag_query(
        self,
        query: str,
        limit: int = 5,
        score_threshold: float = 0.3,
        selected_text: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -> Tuple[str, List[Dict[str, Any]], Dict[str, Any]]:
        """
        Complete RAG processing: search + generate response
        """
        import time
        start_time = time.time()

        # Search for relevant content (unless using selected text only)
        if not selected_text:
            context_chunks = await self.search_content(
                query=query,
                limit=limit,
                score_threshold=score_threshold
            )
        else:
            # For selected text mode, create a single chunk
            context_chunks = [{
                'id': 'selected_text',
                'content': selected_text,
                'metadata': {'source': 'user_selected_text'},
                'score': 1.0
            }]

        # Generate response
        response, metadata = await self.generate_response(
            query=query,
            context_chunks=context_chunks,
            conversation_history=conversation_history,
            selected_text=selected_text
        )

        processing_time = time.time() - start_time

        return response, context_chunks, {
            **metadata,
            'processing_time': processing_time
        }
```

#### Content Service
**backend/app/services/content_service.py:**
```python
import logging
from typing import List, Dict, Any, Optional
from app.config import settings
import openai

logger = logging.getLogger(__name__)

class ContentService:
    def __init__(self):
        openai.api_key = settings.OPENAI_API_KEY

    async def generate_content(
        self,
        topic: str,
        style: str = "educational",
        length: str = "medium",
        target_audience: str = "intermediate",
        context: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Generate educational content using AI
        """
        try:
            prompt = self._build_content_prompt(
                topic=topic,
                style=style,
                length=length,
                target_audience=target_audience,
                context=context
            )

            response = openai.ChatCompletion.create(
                model=settings.OPENAI_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=1000
            )

            content = response.choices[0].message.content

            return {
                'content': content,
                'metadata': {
                    'topic': topic,
                    'style': style,
                    'target_audience': target_audience,
                    'tokens_used': response.usage.total_tokens if response.usage else 0
                }
            }

        except Exception as e:
            logger.error(f"Error in generate_content: {str(e)}")
            raise

    def _build_content_prompt(
        self,
        topic: str,
        style: str,
        length: str,
        target_audience: str,
        context: Optional[str] = None
    ) -> str:
        """
        Build prompt for content generation
        """
        base_prompt = f"""
        Write educational content about "{topic}" for {target_audience} level learners.
        Write in a {style} style and make it {length} length.

        Include these sections:
        1. Hero Insight: [1-2 line emotional opening]
        2. Purpose: [Clear statement of learning objective]
        3. Deep Breakdown: [Main content with clear subsections]
        4. UI/UX Angle: [Visual translation when needed]
        5. Real-World Example: [Short, high-quality case]
        6. Common Mistakes & Fixes: [Practical troubleshooting]
        7. Micro-Exercises: [Small actionable tasks]
        8. End-of-Section Summary: [5-7 bullet takeaways]
        """

        if context:
            base_prompt = f"{base_prompt}\n\nContext: {context}"

        return base_prompt
```

### API Routes Implementation

#### Chat Routes
**backend/app/routes/chat.py:**
```python
from fastapi import APIRouter, Depends, HTTPException
from typing import List
import time
import logging
from app.schemas.chat import ChatRequest, ChatResponse, ChatStreamChunk
from app.schemas.rag import RAGRequest, RAGResponse
from app.services.rag_service import RAGService
from app.config import settings

router = APIRouter()
logger = logging.getLogger(__name__)

@router.post("/", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """
    Main chat endpoint that handles user queries
    """
    start_time = time.time()

    try:
        rag_service = RAGService()

        response, sources, metadata = await rag_service.process_rag_query(
            query=request.message,
            selected_text=request.selected_text,
            conversation_history=[
                {'role': msg.role, 'content': msg.content}
                for msg in request.conversation_history
            ]
        )

        processing_time = time.time() - start_time

        return ChatResponse(
            response=response,
            sources=sources,
            query=request.message,
            tokens_used=metadata.get('tokens_used', 0),
            processing_time=processing_time
        )

    except Exception as e:
        logger.error(f"Chat endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/stream")
async def chat_stream_endpoint():
    """
    Streaming chat endpoint for real-time responses
    """
    # Implementation for streaming responses would go here
    # This would use Server-Sent Events (SSE) or WebSockets
    pass
```

#### RAG Routes
**backend/app/routes/rag.py:**
```python
from fastapi import APIRouter, HTTPException
import time
import logging
from app.schemas.rag import SearchRequest, SearchResponse, RAGRequest, RAGResponse
from app.services.rag_service import RAGService

router = APIRouter()
logger = logging.getLogger(__name__)

@router.post("/search", response_model=SearchResponse)
async def search_endpoint(request: SearchRequest):
    """
    Semantic search endpoint
    """
    start_time = time.time()

    try:
        rag_service = RAGService()

        results = await rag_service.search_content(
            query=request.query,
            limit=request.limit,
            score_threshold=request.score_threshold,
            filters=request.filters
        )

        processing_time = time.time() - start_time

        return SearchResponse(
            results=results,
            query=request.query,
            processing_time=processing_time
        )

    except Exception as e:
        logger.error(f"Search endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/answer", response_model=RAGResponse)
async def rag_answer_endpoint(request: RAGRequest):
    """
    Complete RAG answer generation endpoint
    """
    start_time = time.time()

    try:
        rag_service = RAGService()

        response, sources, metadata = await rag_service.process_rag_query(
            query=request.query,
            limit=request.max_results,
            selected_text=request.selected_text
        )

        processing_time = time.time() - start_time

        return RAGResponse(
            answer=response,
            sources=[result['metadata'] for result in sources],
            query=request.query,
            confidence_score=metadata.get('confidence', 0.8),
            processing_time=processing_time
        )

    except Exception as e:
        logger.error(f"RAG answer endpoint error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
```

### Performance and Optimization

#### Caching Implementation
```python
import redis
import hashlib
import pickle
from functools import wraps

class CacheService:
    def __init__(self, redis_url: str = "redis://localhost:6379"):
        self.redis_client = redis.from_url(redis_url)

    def get_cache_key(self, prefix: str, *args, **kwargs) -> str:
        """Generate cache key from function arguments"""
        key_data = f"{prefix}:{str(args)}:{str(kwargs)}"
        return f"cache:{hashlib.md5(key_data.encode()).hexdigest()}"

    def cached(self, ttl: int = 300):
        """Decorator for caching function results"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                cache_key = self.get_cache_key(func.__name__, *args, **kwargs)

                # Try to get from cache
                cached_result = self.redis_client.get(cache_key)
                if cached_result:
                    return pickle.loads(cached_result)

                # Execute function
                result = await func(*args, **kwargs)

                # Cache result
                self.redis_client.setex(
                    cache_key,
                    ttl,
                    pickle.dumps(result)
                )

                return result
            return wrapper
        return decorator

# Usage in services
cache_service = CacheService()

class RAGService:
    # ... existing code ...

    @cache_service.cached(ttl=600)  # Cache for 10 minutes
    async def search_content(self, query: str, limit: int = 5, score_threshold: float = 0.3):
        # ... existing search logic ...
        pass
```

#### Rate Limiting
```python
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from fastapi import Request

limiter = Limiter(key_func=get_remote_address)

app.state.limiter = limiter
app.add_exception_handler(429, _rate_limit_exceeded_handler)

@router.post("/", response_model=ChatResponse)
@limiter.limit("10/minute")  # 10 requests per minute per IP
async def chat_endpoint(request: ChatRequest, request_obj: Request):
    # ... existing implementation ...
    pass
```

## üé® UI/UX Angle
The backend performance directly impacts the user experience. Fast response times, proper error handling, and reliable service availability are crucial for maintaining a positive user experience with the AI assistant.

## üìò Real-World Example
A production FastAPI backend handles:
1. High-volume chat requests during peak usage
2. Complex RAG operations across large documentation sets
3. Proper error handling and fallback mechanisms
4. Performance monitoring and optimization
5. Secure API key management and rate limiting

## ‚ö†Ô∏è Common Mistakes & Fixes

### Mistake: Not implementing proper error handling and logging
**Fix**: Add comprehensive exception handling and structured logging throughout the application.

### Mistake: Hardcoding configuration values instead of using environment variables
**Fix**: Use Pydantic Settings for all configuration management.

### Mistake: Not validating input data properly
**Fix**: Use Pydantic models for comprehensive request validation.

## üß™ Micro-Exercises

1. **Exercise**: Set up the FastAPI project structure with proper routing and models.
2. **Exercise**: Implement a basic RAG service with search and response generation.

## üß≠ Reflection Prompts
- How does your current backend architecture handle AI service integration?
- What challenges have you faced with API performance and reliability?
- How might proper backend architecture improve your AI application?

## End-of-Chapter Summary
- FastAPI provides a robust foundation for AI backend services
- Proper architecture includes multiple service layers
- Configuration management is crucial for deployment flexibility
- Performance optimization ensures good user experience
- The backend architecture enables scalable AI documentation systems