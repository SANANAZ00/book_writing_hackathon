"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[6465],{8453:(n,e,i)=>{i.d(e,{R:()=>r});var o=i(6540);const t={},s=o.createContext(t);function r(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}},9080:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>s,metadata:()=>a,toc:()=>c});var o=i(4848),t=i(8453);const s={title:"Module 3 - Digital Twin (Gazebo & Unity)",sidebar_position:3,description:"Physics simulation for humanoid robotics in Gazebo and Unity",tags:["gazebo","unity","simulation","digital-twin","physics"]},r="Module 3: Digital Twin (Gazebo & Unity)",a={id:"module-3-digital-twin",title:"Module 3 - Digital Twin (Gazebo & Unity)",description:"Physics simulation for humanoid robotics in Gazebo and Unity",source:"@site/docs/module-3-digital-twin.mdx",sourceDirName:".",slug:"/module-3-digital-twin",permalink:"/book_writing_hackathon/docs/module-3-digital-twin",draft:!1,unlisted:!1,editUrl:"https://github.com/SANANAZ00/book_writing_hackathon/edit/main/docs/module-3-digital-twin.mdx",tags:[{label:"gazebo",permalink:"/book_writing_hackathon/docs/tags/gazebo"},{label:"unity",permalink:"/book_writing_hackathon/docs/tags/unity"},{label:"simulation",permalink:"/book_writing_hackathon/docs/tags/simulation"},{label:"digital-twin",permalink:"/book_writing_hackathon/docs/tags/digital-twin"},{label:"physics",permalink:"/book_writing_hackathon/docs/tags/physics"}],version:"current",sidebarPosition:3,frontMatter:{title:"Module 3 - Digital Twin (Gazebo & Unity)",sidebar_position:3,description:"Physics simulation for humanoid robotics in Gazebo and Unity",tags:["gazebo","unity","simulation","digital-twin","physics"]},sidebar:"tutorialSidebar",previous:{title:"Module 2 - ROS 2 (Robotic Nervous System)",permalink:"/book_writing_hackathon/docs/module-2-ros2"},next:{title:"Module 4 - AI-Robot Brain (NVIDIA Isaac)",permalink:"/book_writing_hackathon/docs/module-4-ai-brain"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"1. Introduction to Digital Twins in Robotics",id:"1-introduction-to-digital-twins-in-robotics",level:2},{value:"Why Simulation is Critical for Humanoid Robotics",id:"why-simulation-is-critical-for-humanoid-robotics",level:3},{value:"2. Gazebo: Physics-Based Simulation for Robotics",id:"2-gazebo-physics-based-simulation-for-robotics",level:2},{value:"2.1 Gazebo Architecture and Features",id:"21-gazebo-architecture-and-features",level:3},{value:"2.2 Setting Up a Humanoid Robot in Gazebo",id:"22-setting-up-a-humanoid-robot-in-gazebo",level:3},{value:"2.3 Creating Gazebo Worlds",id:"23-creating-gazebo-worlds",level:3},{value:"2.4 Physics Simulation for Humanoid Robotics",id:"24-physics-simulation-for-humanoid-robotics",level:3},{value:"3. Unity: Realistic Environment Rendering",id:"3-unity-realistic-environment-rendering",level:2},{value:"3.1 Unity Robotics Hub",id:"31-unity-robotics-hub",level:3},{value:"3.2 Creating Photorealistic Environments",id:"32-creating-photorealistic-environments",level:3},{value:"3.3 Unity-Rosbridge Integration",id:"33-unity-rosbridge-integration",level:3},{value:"4. Sensor Integration in Simulation Platforms",id:"4-sensor-integration-in-simulation-platforms",level:2},{value:"4.1 Gazebo Sensor Integration",id:"41-gazebo-sensor-integration",level:3},{value:"4.2 Unity Sensor Integration",id:"42-unity-sensor-integration",level:3},{value:"5. Comparison: Gazebo vs Unity for Different Use Cases",id:"5-comparison-gazebo-vs-unity-for-different-use-cases",level:2},{value:"When to Use Gazebo",id:"when-to-use-gazebo",level:3},{value:"When to Use Unity",id:"when-to-use-unity",level:3},{value:"Hybrid Approach",id:"hybrid-approach",level:3},{value:"6. Practical Exercise: Creating a Simple Humanoid Simulation",id:"6-practical-exercise-creating-a-simple-humanoid-simulation",level:2},{value:"Exercise 6.1: Basic Humanoid Model in Gazebo",id:"exercise-61-basic-humanoid-model-in-gazebo",level:3},{value:"Exercise 6.2: Launch File for the Simulation",id:"exercise-62-launch-file-for-the-simulation",level:3},{value:"Exercise 6.3: Control Node",id:"exercise-63-control-node",level:3},{value:"7. Summary",id:"7-summary",level:2},{value:"Assessment",id:"assessment",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.h1,{id:"module-3-digital-twin-gazebo--unity",children:"Module 3: Digital Twin (Gazebo & Unity)"}),"\n",(0,o.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(e.p,{children:"After completing this module, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Explain physics simulation for humanoid robotics in Gazebo"}),"\n",(0,o.jsx)(e.li,{children:"Demonstrate realistic environment rendering in Unity"}),"\n",(0,o.jsx)(e.li,{children:"Show sensor integration in both simulation platforms"}),"\n",(0,o.jsx)(e.li,{children:"Provide comparison between Gazebo and Unity for different use cases"}),"\n",(0,o.jsx)(e.li,{children:"Create and test humanoid robot simulations in both environments"}),"\n",(0,o.jsx)(e.li,{children:"Implement sensor data processing pipelines for simulated robots"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Completion of Module 1: Introduction"}),"\n",(0,o.jsx)(e.li,{children:"Completion of Module 2: ROS 2 (Robotic Nervous System)"}),"\n",(0,o.jsx)(e.li,{children:"Basic understanding of 3D modeling concepts"}),"\n",(0,o.jsx)(e.li,{children:"Familiarity with Linux (for Gazebo) and Windows/Mac (for Unity)"}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"1-introduction-to-digital-twins-in-robotics",children:"1. Introduction to Digital Twins in Robotics"}),"\n",(0,o.jsx)(e.p,{children:"A digital twin in robotics is a virtual replica of a physical robot that exists in simulation environments. This virtual model mirrors the physical robot's behavior, allowing for testing, validation, and optimization without the risks and costs associated with real-world experimentation."}),"\n",(0,o.jsx)(e.p,{children:"For humanoid robotics, digital twins are particularly valuable because they enable:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Safe testing of complex behaviors before deployment"}),"\n",(0,o.jsx)(e.li,{children:"Rapid iteration on control algorithms"}),"\n",(0,o.jsx)(e.li,{children:"Training of AI models in diverse scenarios"}),"\n",(0,o.jsx)(e.li,{children:"Validation of sensor integration and perception systems"}),"\n",(0,o.jsx)(e.li,{children:"Stress testing under various environmental conditions"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"The concept of digital twins has become increasingly important as robotics systems become more complex and expensive. Rather than testing every possible scenario on expensive physical hardware, engineers can validate their systems in simulation first, dramatically reducing development time and costs."}),"\n",(0,o.jsx)(e.h3,{id:"why-simulation-is-critical-for-humanoid-robotics",children:"Why Simulation is Critical for Humanoid Robotics"}),"\n",(0,o.jsx)(e.p,{children:"Humanoid robots operate in human-designed environments and must interact safely with humans. This creates unique challenges that are difficult to address without extensive simulation:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Safety"}),": Humanoid robots must operate safely around people, requiring extensive testing of emergency procedures"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Complexity"}),": With dozens of joints and multiple sensory systems, humanoid robots have complex dynamics"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cost"}),": Physical humanoid robots are expensive, making extensive real-world testing impractical"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Repeatability"}),": Simulation allows for repeatable experiments with controlled variables"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"2-gazebo-physics-based-simulation-for-robotics",children:"2. Gazebo: Physics-Based Simulation for Robotics"}),"\n",(0,o.jsx)(e.p,{children:"Gazebo is a physics-based simulation environment that provides realistic robot simulation and testing. It's widely used in the robotics community and integrates seamlessly with ROS 2, making it an ideal choice for humanoid robotics development."}),"\n",(0,o.jsx)(e.h3,{id:"21-gazebo-architecture-and-features",children:"2.1 Gazebo Architecture and Features"}),"\n",(0,o.jsx)(e.p,{children:"Gazebo provides several key features essential for humanoid robotics:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Realistic Physics Engine"}),": Based on ODE (Open Dynamics Engine), Bullet, or DART, providing accurate simulation of rigid body dynamics"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensor Simulation"}),": Support for cameras, LIDAR, IMUs, force/torque sensors, and other robot sensors"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Environment Modeling"}),": Tools for creating complex 3D environments with realistic lighting and materials"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"ROS 2 Integration"}),": Native support for ROS 2 topics, services, and actions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Plugin Architecture"}),": Extensible system for custom sensors, controllers, and world modifications"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"22-setting-up-a-humanoid-robot-in-gazebo",children:"2.2 Setting Up a Humanoid Robot in Gazebo"}),"\n",(0,o.jsx)(e.p,{children:"To use a humanoid robot in Gazebo, you need to create a URDF model that includes Gazebo-specific extensions. These extensions define how the robot interacts with the physics engine and how sensors are simulated."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Example: URDF with Gazebo extensions for a humanoid robot --\x3e\n<?xml version="1.0"?>\n<robot name="humanoid_with_gazebo_extensions">\n  \x3c!-- Include your basic URDF model here --\x3e\n\n  \x3c!-- Gazebo-specific extensions --\x3e\n  <gazebo reference="base_link">\n    <material>Gazebo/Blue</material>\n    <mu1>0.2</mu1>\n    <mu2>0.2</mu2>\n  </gazebo>\n\n  \x3c!-- Camera sensor --\x3e\n  <gazebo reference="camera_link">\n    <sensor type="camera" name="camera1">\n      <update_rate>30.0</update_rate>\n      <camera name="head">\n        <horizontal_fov>1.3962634</horizontal_fov>\n        <image>\n          <width>800</width>\n          <height>600</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.02</near>\n          <far>300</far>\n        </clip>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n        <frame_name>camera_optical_frame</frame_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- IMU sensor --\x3e\n  <gazebo reference="imu_link">\n    <sensor type="imu" name="imu_sensor">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <visualize>true</visualize>\n      <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\n        <topic>__default_topic__</topic>\n        <body_name>imu_link</body_name>\n        <update_rate>100</update_rate>\n        <gaussian_noise>0.01</gaussian_noise>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- ROS 2 Control interface --\x3e\n  <gazebo>\n    <plugin filename="libgazebo_ros2_control.so" name="gazebo_ros2_control">\n      <parameters>$(find my_robot_description)/config/my_robot_controllers.yaml</parameters>\n    </plugin>\n  </gazebo>\n</robot>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"23-creating-gazebo-worlds",children:"2.3 Creating Gazebo Worlds"}),"\n",(0,o.jsx)(e.p,{children:"Gazebo worlds define the environment in which your robot operates. These are SDF (Simulation Description Format) files that specify the physics properties, lighting, and objects in the environment."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Example: Simple Gazebo world --\x3e\n<?xml version="1.0" ?>\n<sdf version="1.6">\n  <world name="simple_world">\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n\n    <include>\n      <uri>model://sun</uri>\n    </include>\n\n    \x3c!-- Add your robot --\x3e\n    <include>\n      <uri>model://humanoid_robot</uri>\n    </include>\n\n    \x3c!-- Add obstacles --\x3e\n    <model name="table">\n      <pose>2 0 0.5 0 0 0</pose>\n      <link name="link">\n        <collision name="collision">\n          <geometry>\n            <box>\n              <size>1 0.8 0.8</size>\n            </box>\n          </geometry>\n        </collision>\n        <visual name="visual">\n          <geometry>\n            <box>\n              <size>1 0.8 0.8</size>\n            </box>\n          </geometry>\n        </visual>\n        <inertial>\n          <mass>10</mass>\n          <inertia>\n            <ixx>1</ixx>\n            <ixy>0</ixy>\n            <ixz>0</ixz>\n            <iyy>1</iyy>\n            <iyz>0</iyz>\n            <izz>1</izz>\n          </inertia>\n        </inertial>\n      </link>\n    </model>\n  </world>\n</sdf>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"24-physics-simulation-for-humanoid-robotics",children:"2.4 Physics Simulation for Humanoid Robotics"}),"\n",(0,o.jsx)(e.p,{children:"Physics simulation in Gazebo is crucial for humanoid robots because it accurately models the complex interactions between the robot and its environment. This includes:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Contact Dynamics"}),": How the robot's feet interact with the ground during walking"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Balance and Stability"}),": How the robot maintains balance under various conditions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Manipulation Physics"}),": How the robot interacts with objects in the environment"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensor Accuracy"}),": How sensors respond to environmental conditions"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"The physics engine calculates forces, torques, and resulting motions in real-time, providing realistic feedback for control algorithms. This allows developers to test complex behaviors like walking, grasping, and navigation in a safe, repeatable environment."}),"\n",(0,o.jsx)(e.h2,{id:"3-unity-realistic-environment-rendering",children:"3. Unity: Realistic Environment Rendering"}),"\n",(0,o.jsx)(e.p,{children:"Unity is a powerful game engine that has been adapted for robotics simulation through the Unity Robotics package. Unlike Gazebo, which focuses on physics accuracy, Unity excels at creating photorealistic environments with advanced rendering capabilities."}),"\n",(0,o.jsx)(e.h3,{id:"31-unity-robotics-hub",children:"3.1 Unity Robotics Hub"}),"\n",(0,o.jsx)(e.p,{children:"Unity provides several tools specifically for robotics:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Unity Robotics Hub"}),": Centralized package management for robotics tools"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Unity ML-Agents"}),": Framework for training AI agents in Unity environments"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"ROS#"}),": Bridge for connecting Unity to ROS/ROS 2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Robotics Object Detection"}),": Tools for training computer vision models"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Synthetic Data Generation"}),": Creating labeled datasets for training"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"32-creating-photorealistic-environments",children:"3.2 Creating Photorealistic Environments"}),"\n",(0,o.jsx)(e.p,{children:"Unity's strength lies in creating visually realistic environments that closely match real-world conditions. This is particularly valuable for:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Computer Vision Training"}),": Creating realistic images for training perception models"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Human-Robot Interaction"}),": Simulating realistic lighting and environmental conditions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensor Simulation"}),": Testing sensors under various lighting and environmental conditions"]}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Example: Unity script for robot control integration\nusing UnityEngine;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Std;\n\npublic class RobotController : MonoBehaviour\n{\n    ROSConnection ros;\n    string robotTopic = "unity_robot_command";\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n        ros.RegisterPublisher<UInt8Msg>(robotTopic);\n    }\n\n    void Update()\n    {\n        // Example: Send a command to the robot every second\n        if (Time.time % 1.0f < Time.deltaTime)\n        {\n            UInt8Msg command = new UInt8Msg();\n            command.data = 1; // Move forward command\n            ros.Publish(robotTopic, command);\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"33-unity-rosbridge-integration",children:"3.3 Unity-Rosbridge Integration"}),"\n",(0,o.jsx)(e.p,{children:"Unity connects to ROS 2 through the ROS-TCP-Connector, which provides a bridge between Unity's C# environment and ROS 2's communication system. This allows:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Real-time Communication"}),": Low-latency communication between Unity and ROS 2 nodes"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensor Data Streaming"}),": Real-time sensor data from Unity to ROS 2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Control Command Transmission"}),": Sending control commands from ROS 2 to Unity robots"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Synchronized Simulation"}),": Keeping Unity physics synchronized with ROS 2 time"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"4-sensor-integration-in-simulation-platforms",children:"4. Sensor Integration in Simulation Platforms"}),"\n",(0,o.jsx)(e.p,{children:"Both Gazebo and Unity provide comprehensive sensor simulation capabilities, but with different strengths and approaches."}),"\n",(0,o.jsx)(e.h3,{id:"41-gazebo-sensor-integration",children:"4.1 Gazebo Sensor Integration"}),"\n",(0,o.jsx)(e.p,{children:"Gazebo offers native sensor plugins with realistic physics-based simulation:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Camera Sensors"}),": Simulates RGB, depth, and stereo cameras with realistic distortion"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"LIDAR Sensors"}),": Simulates 2D and 3D LIDAR with configurable resolution and range"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"IMU Sensors"}),": Simulates accelerometers and gyroscopes with configurable noise"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Force/Torque Sensors"}),": Simulates joint forces and external forces"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"GPS Sensors"}),": Simulates GPS with configurable accuracy and noise"]}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- Example: Advanced camera sensor configuration --\x3e\n<gazebo reference="camera_link">\n  <sensor type="camera" name="front_camera">\n    <update_rate>30</update_rate>\n    <camera name="head">\n      <horizontal_fov>1.047</horizontal_fov>\n      <image>\n        <width>640</width>\n        <height>480</height>\n        <format>R8G8B8</format>\n      </image>\n      <clip>\n        <near>0.1</near>\n        <far>100</far>\n      </clip>\n      <noise>\n        <type>gaussian</type>\n        <mean>0.0</mean>\n        <stddev>0.007</stddev>\n      </noise>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <frame_name>camera_optical_frame</frame_name>\n      <min_depth>0.1</min_depth>\n      <max_depth>100.0</max_depth>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"42-unity-sensor-integration",children:"4.2 Unity Sensor Integration"}),"\n",(0,o.jsx)(e.p,{children:"Unity provides sensor simulation through its rendering and physics systems:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"RGB Cameras"}),": High-quality rendering with realistic lighting and materials"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Depth Cameras"}),": Realistic depth perception based on raycasting"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Semantic Segmentation"}),": Pixel-level object classification in rendered images"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Instance Segmentation"}),": Individual object identification in scenes"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"LiDAR Simulation"}),": Raycasting-based LiDAR simulation with realistic noise"]}),"\n"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-csharp",children:'// Example: Unity sensor data collection\nusing UnityEngine;\nusing System.Collections;\nusing Unity.Robotics.ROSTCPConnector;\nusing RosMessageTypes.Sensor;\n\npublic class UnityCameraSensor : MonoBehaviour\n{\n    public Camera sensorCamera;\n    private ROSConnection ros;\n    private string imageTopic = "unity_camera/image_raw";\n\n    void Start()\n    {\n        ros = ROSConnection.GetOrCreateInstance();\n    }\n\n    void Update()\n    {\n        if (Time.time % 0.1f < Time.deltaTime) // Publish at 10Hz\n        {\n            // Capture image from camera\n            RenderTexture currentRT = RenderTexture.active;\n            RenderTexture.active = sensorCamera.targetTexture;\n            sensorCamera.Render();\n\n            Texture2D image = new Texture2D(sensorCamera.targetTexture.width, sensorCamera.targetTexture.height);\n            image.ReadPixels(new Rect(0, 0, sensorCamera.targetTexture.width, sensorCamera.targetTexture.height), 0, 0);\n            image.Apply();\n\n            RenderTexture.active = currentRT;\n\n            // Convert and publish image (simplified)\n            // In practice, you would convert to ROS image format\n        }\n    }\n}\n'})}),"\n",(0,o.jsx)(e.h2,{id:"5-comparison-gazebo-vs-unity-for-different-use-cases",children:"5. Comparison: Gazebo vs Unity for Different Use Cases"}),"\n",(0,o.jsx)(e.h3,{id:"when-to-use-gazebo",children:"When to Use Gazebo"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Strengths:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Accurate physics simulation"}),"\n",(0,o.jsx)(e.li,{children:"Native ROS 2 integration"}),"\n",(0,o.jsx)(e.li,{children:"Extensive robotics tooling"}),"\n",(0,o.jsx)(e.li,{children:"Large community and resources"}),"\n",(0,o.jsx)(e.li,{children:"Realistic sensor simulation"}),"\n",(0,o.jsx)(e.li,{children:"Open source and free"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Best for:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Physics-based testing and validation"}),"\n",(0,o.jsx)(e.li,{children:"Control algorithm development"}),"\n",(0,o.jsx)(e.li,{children:"Realistic sensor simulation"}),"\n",(0,o.jsx)(e.li,{children:"Standard robotics research"}),"\n",(0,o.jsx)(e.li,{children:"Integration with existing ROS 2 workflows"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"when-to-use-unity",children:"When to Use Unity"}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Strengths:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Photorealistic rendering"}),"\n",(0,o.jsx)(e.li,{children:"Advanced graphics capabilities"}),"\n",(0,o.jsx)(e.li,{children:"High-quality environment creation"}),"\n",(0,o.jsx)(e.li,{children:"Machine learning integration"}),"\n",(0,o.jsx)(e.li,{children:"Cross-platform deployment"}),"\n",(0,o.jsx)(e.li,{children:"Professional game engine features"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Best for:"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Computer vision training"}),"\n",(0,o.jsx)(e.li,{children:"Human-robot interaction studies"}),"\n",(0,o.jsx)(e.li,{children:"Photorealistic simulation"}),"\n",(0,o.jsx)(e.li,{children:"ML agent training"}),"\n",(0,o.jsx)(e.li,{children:"User interface development"}),"\n",(0,o.jsx)(e.li,{children:"Presentation and demonstration"}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"hybrid-approach",children:"Hybrid Approach"}),"\n",(0,o.jsx)(e.p,{children:"Many advanced robotics projects use both platforms:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Gazebo"})," for physics-accurate control development"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Unity"})," for perception and vision system training"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cross-platform validation"})," to ensure consistency"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"6-practical-exercise-creating-a-simple-humanoid-simulation",children:"6. Practical Exercise: Creating a Simple Humanoid Simulation"}),"\n",(0,o.jsx)(e.p,{children:"Let's create a simple humanoid simulation in Gazebo that demonstrates the concepts we've learned."}),"\n",(0,o.jsx)(e.h3,{id:"exercise-61-basic-humanoid-model-in-gazebo",children:"Exercise 6.1: Basic Humanoid Model in Gazebo"}),"\n",(0,o.jsx)(e.p,{children:"First, create a simple URDF model for a basic humanoid:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- simple_humanoid.urdf --\x3e\n<?xml version="1.0"?>\n<robot name="simple_humanoid">\n  \x3c!-- Base body --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <box size="0.2 0.1 0.4"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.2 0.1 0.4"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="10.0"/>\n      <inertia ixx="1.0" ixy="0.0" ixz="0.0" iyy="1.0" iyz="0.0" izz="1.0"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Head --\x3e\n  <link name="head">\n    <visual>\n      <geometry>\n        <sphere radius="0.08"/>\n      </geometry>\n      <material name="white">\n        <color rgba="1 1 1 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <sphere radius="0.08"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="2.0"/>\n      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  <joint name="neck_joint" type="revolute">\n    <parent link="base_link"/>\n    <child link="head"/>\n    <origin xyz="0 0 0.25"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-0.5" upper="0.5" effort="100" velocity="1"/>\n  </joint>\n\n  \x3c!-- Left arm --\x3e\n  <link name="left_upper_arm">\n    <visual>\n      <geometry>\n        <cylinder length="0.2" radius="0.03"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 1"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.2" radius="0.03"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.01" ixy="0.0" ixz="0.0" iyy="0.01" iyz="0.0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  <joint name="left_shoulder_joint" type="revolute">\n    <parent link="base_link"/>\n    <child link="left_upper_arm"/>\n    <origin xyz="0.15 0 0.1" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="50" velocity="1"/>\n  </joint>\n\n  \x3c!-- Add Gazebo extensions --\x3e\n  <gazebo>\n    <plugin name="joint_state_publisher" filename="libgazebo_ros_joint_state_publisher.so">\n      <joint_name>neck_joint, left_shoulder_joint</joint_name>\n    </plugin>\n  </gazebo>\n</robot>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"exercise-62-launch-file-for-the-simulation",children:"Exercise 6.2: Launch File for the Simulation"}),"\n",(0,o.jsx)(e.p,{children:"Create a launch file to start the simulation:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-xml",children:'\x3c!-- launch/humanoid_simulation.launch.xml --\x3e\n<launch>\n  \x3c!-- Load the robot description --\x3e\n  <param name="robot_description"\n         value="$(find-pkg-share my_robot_description)/urdf/simple_humanoid.urdf"/>\n\n  \x3c!-- Start robot state publisher --\x3e\n  <node pkg="robot_state_publisher"\n        exec="robot_state_publisher"\n        name="robot_state_publisher">\n    <param name="robot_description"\n           value="$(var robot_description)"/>\n  </node>\n\n  \x3c!-- Start Gazebo --\x3e\n  <include file="$(find-pkg-share gazebo_ros)/launch/gazebo.launch.py">\n    <arg name="world" value="$(find-pkg-share my_robot_description)/worlds/simple_world.world"/>\n  </include>\n\n  \x3c!-- Spawn the robot in Gazebo --\x3e\n  <node pkg="gazebo_ros"\n        exec="spawn_entity.py"\n        args="-topic robot_description -entity simple_humanoid"/>\n</launch>\n'})}),"\n",(0,o.jsx)(e.h3,{id:"exercise-63-control-node",children:"Exercise 6.3: Control Node"}),"\n",(0,o.jsx)(e.p,{children:"Create a simple control node to move the joints:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# control_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\nfrom trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint\nimport math\n\nclass SimpleController(Node):\n    def __init__(self):\n        super().__init__('simple_controller')\n\n        # Publisher for joint trajectory commands\n        self.trajectory_pub = self.create_publisher(\n            JointTrajectory,\n            '/joint_trajectory',\n            10\n        )\n\n        # Timer for sending commands\n        self.timer = self.create_timer(0.1, self.timer_callback)\n        self.time = 0.0\n\n    def timer_callback(self):\n        trajectory_msg = JointTrajectory()\n        trajectory_msg.joint_names = ['neck_joint', 'left_shoulder_joint']\n\n        point = JointTrajectoryPoint()\n        point.positions = [\n            0.5 * math.sin(self.time),  # neck joint\n            0.5 * math.cos(self.time)   # left shoulder joint\n        ]\n        point.time_from_start.sec = 0\n        point.time_from_start.nanosec = 100000000  # 0.1 seconds\n\n        trajectory_msg.points = [point]\n\n        self.trajectory_pub.publish(trajectory_msg)\n        self.time += 0.1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    controller = SimpleController()\n    rclpy.spin(controller)\n    controller.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(e.h2,{id:"7-summary",children:"7. Summary"}),"\n",(0,o.jsx)(e.p,{children:"In this module, you've learned about digital twin simulation for humanoid robotics using both Gazebo and Unity. You now understand:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"How to create and configure humanoid robot models in Gazebo"}),"\n",(0,o.jsx)(e.li,{children:"The process of setting up realistic physics simulation environments"}),"\n",(0,o.jsx)(e.li,{children:"How to integrate Unity for photorealistic rendering and perception training"}),"\n",(0,o.jsx)(e.li,{children:"The strengths and use cases for each simulation platform"}),"\n",(0,o.jsx)(e.li,{children:"How to implement sensor integration in both environments"}),"\n",(0,o.jsx)(e.li,{children:"Best practices for simulation-based robot development"}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"Simulation is a critical tool in the development of humanoid robots, allowing for safe, cost-effective testing and validation of complex behaviors. The choice between Gazebo and Unity (or using both) depends on your specific needs and use cases."}),"\n",(0,o.jsx)(e.p,{children:"In the next module, we'll explore the NVIDIA Isaac platform, which provides specialized tools for AI-robot integration, including perception and navigation systems."}),"\n",(0,o.jsx)(e.h2,{id:"assessment",children:"Assessment"}),"\n",(0,o.jsx)(e.p,{children:"Complete the following exercises to reinforce your understanding:"}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Create a Gazebo world with obstacles and test your humanoid robot's navigation capabilities"}),"\n",(0,o.jsx)(e.li,{children:"Implement a Unity scene with realistic lighting and train a simple object detection model"}),"\n",(0,o.jsx)(e.li,{children:"Add IMU and camera sensors to your humanoid model and simulate sensor data"}),"\n",(0,o.jsx)(e.li,{children:"Compare the physics behavior of your robot in Gazebo vs Unity for the same control commands"}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,t.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}}}]);