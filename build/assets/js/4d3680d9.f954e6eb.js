"use strict";(globalThis.webpackChunkai_book=globalThis.webpackChunkai_book||[]).push([[609],{7312:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>i,metadata:()=>o,toc:()=>l});var s=t(4848),r=t(8453);const i={sidebar_position:4},a="FastAPI Backend Setup and Architecture",o={id:"systems/fastapi-backend",title:"FastAPI Backend Setup and Architecture",description:"Hero Insight",source:"@site/docs/systems/fastapi-backend.mdx",sourceDirName:"systems",slug:"/systems/fastapi-backend",permalink:"/book_writing_hackathon/docs/systems/fastapi-backend",draft:!1,unlisted:!1,editUrl:"https://github.com/SANANAZ00/book_writing_hackathon/edit/main/docs/systems/fastapi-backend.mdx",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Building the RAG Chatbot Foundation",permalink:"/book_writing_hackathon/docs/systems/building-rag-chatbot"}},c={},l=[{value:"Hero Insight",id:"hero-insight",level:2},{value:"Purpose of This Chapter",id:"purpose-of-this-chapter",level:2},{value:"Deep Breakdown",id:"deep-breakdown",level:2},{value:"FastAPI Backend Architecture Overview",id:"fastapi-backend-architecture-overview",level:3},{value:"1. API Layer",id:"1-api-layer",level:4},{value:"2. Service Layer",id:"2-service-layer",level:4},{value:"3. Data Layer",id:"3-data-layer",level:4},{value:"4. Utility Layer",id:"4-utility-layer",level:4},{value:"Project Structure",id:"project-structure",level:3},{value:"Core FastAPI Application Setup",id:"core-fastapi-application-setup",level:3},{value:"Main Application File",id:"main-application-file",level:4},{value:"Configuration Management",id:"configuration-management",level:4},{value:"Data Models and Schemas",id:"data-models-and-schemas",level:3},{value:"Request and Response Models",id:"request-and-response-models",level:4},{value:"Service Layer Implementation",id:"service-layer-implementation",level:3},{value:"RAG Service",id:"rag-service",level:4},{value:"Content Service",id:"content-service",level:4},{value:"API Routes Implementation",id:"api-routes-implementation",level:3},{value:"Chat Routes",id:"chat-routes",level:4},{value:"RAG Routes",id:"rag-routes",level:4},{value:"Performance and Optimization",id:"performance-and-optimization",level:3},{value:"Caching Implementation",id:"caching-implementation",level:4},{value:"Rate Limiting",id:"rate-limiting",level:4},{value:"\ud83c\udfa8 UI/UX Angle",id:"-uiux-angle",level:2},{value:"\ud83d\udcd8 Real-World Example",id:"-real-world-example",level:2},{value:"\u26a0\ufe0f Common Mistakes &amp; Fixes",id:"\ufe0f-common-mistakes--fixes",level:2},{value:"Mistake: Not implementing proper error handling and logging",id:"mistake-not-implementing-proper-error-handling-and-logging",level:3},{value:"Mistake: Hardcoding configuration values instead of using environment variables",id:"mistake-hardcoding-configuration-values-instead-of-using-environment-variables",level:3},{value:"Mistake: Not validating input data properly",id:"mistake-not-validating-input-data-properly",level:3},{value:"\ud83e\uddea Micro-Exercises",id:"-micro-exercises",level:2},{value:"\ud83e\udded Reflection Prompts",id:"-reflection-prompts",level:2},{value:"End-of-Chapter Summary",id:"end-of-chapter-summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"fastapi-backend-setup-and-architecture",children:"FastAPI Backend Setup and Architecture"}),"\n",(0,s.jsx)(n.h2,{id:"hero-insight",children:"Hero Insight"}),"\n",(0,s.jsx)(n.p,{children:"Transform your AI documentation system into a robust, scalable backend service that handles complex RAG operations with efficiency and reliability."}),"\n",(0,s.jsx)(n.h2,{id:"purpose-of-this-chapter",children:"Purpose of This Chapter"}),"\n",(0,s.jsx)(n.p,{children:"You will build a comprehensive FastAPI backend architecture that supports RAG operations, content generation, user management, and all the services needed for your AI-powered documentation system."}),"\n",(0,s.jsx)(n.h2,{id:"deep-breakdown",children:"Deep Breakdown"}),"\n",(0,s.jsx)(n.h3,{id:"fastapi-backend-architecture-overview",children:"FastAPI Backend Architecture Overview"}),"\n",(0,s.jsx)(n.p,{children:"A production-ready FastAPI backend for AI documentation includes multiple layers:"}),"\n",(0,s.jsx)(n.h4,{id:"1-api-layer",children:"1. API Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Route Organization"}),": Well-structured endpoints for different functionalities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Request Validation"}),": Pydantic models for data validation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response Serialization"}),": Consistent response formats"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Proper HTTP status codes and error messages"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"2-service-layer",children:"2. Service Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Business Logic"}),": Core functionality implementations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"External API Integration"}),": OpenAI, Qdrant, and other services"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Caching"}),": Performance optimization strategies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Rate Limiting"}),": API usage control"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"3-data-layer",children:"3. Data Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Model Definitions"}),": Pydantic models for data structures"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Database Abstractions"}),": Qdrant client integration"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Data Validation"}),": Consistent data integrity"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transformation Logic"}),": Data processing and formatting"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"4-utility-layer",children:"4. Utility Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Configuration Management"}),": Environment-based settings"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Logging"}),": Comprehensive system monitoring"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Security"}),": Authentication and authorization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Health Checks"}),": System status monitoring"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"project-structure",children:"Project Structure"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"backend/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 main.py              # FastAPI application instance\n\u2502   \u251c\u2500\u2500 config.py           # Configuration management\n\u2502   \u251c\u2500\u2500 database.py         # Database connections\n\u2502   \u251c\u2500\u2500 models/             # Pydantic models\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2502   \u251c\u2500\u2500 request.py\n\u2502   \u2502   \u2514\u2500\u2500 response.py\n\u2502   \u251c\u2500\u2500 schemas/            # Pydantic schemas\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 chat.py\n\u2502   \u2502   \u251c\u2500\u2500 rag.py\n\u2502   \u2502   \u2514\u2500\u2500 content.py\n\u2502   \u251c\u2500\u2500 services/           # Business logic\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 rag_service.py\n\u2502   \u2502   \u251c\u2500\u2500 content_service.py\n\u2502   \u2502   \u2514\u2500\u2500 embedding_service.py\n\u2502   \u251c\u2500\u2500 routes/             # API endpoints\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 chat.py\n\u2502   \u2502   \u251c\u2500\u2500 rag.py\n\u2502   \u2502   \u2514\u2500\u2500 content.py\n\u2502   \u2514\u2500\u2500 utils/              # Utility functions\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 logging.py\n\u2502       \u251c\u2500\u2500 validation.py\n\u2502       \u2514\u2500\u2500 security.py\n\u251c\u2500\u2500 tests/                  # Test files\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 .env\n"})}),"\n",(0,s.jsx)(n.h3,{id:"core-fastapi-application-setup",children:"Core FastAPI Application Setup"}),"\n",(0,s.jsx)(n.h4,{id:"main-application-file",children:"Main Application File"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"backend/app/main.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from fastapi import FastAPI, Request\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.responses import JSONResponse\nfrom contextlib import asynccontextmanager\nimport logging\nimport time\n\nfrom app.config import settings\nfrom app.routes import chat, rag, content\nfrom app.utils.logging import setup_logging\nfrom app.database import init_db\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    """\n    Application lifespan events\n    """\n    # Startup\n    setup_logging()\n    await init_db()\n    logging.info("Application started")\n    yield\n    # Shutdown\n    logging.info("Application shutting down")\n\napp = FastAPI(\n    title="AI Documentation Backend",\n    description="Backend services for AI-powered documentation system",\n    version="1.0.0",\n    lifespan=lifespan\n)\n\n# Configure CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.ALLOWED_ORIGINS,\n    allow_credentials=True,\n    allow_methods=["*"],\n    allow_headers=["*"],\n)\n\n# Include routers\napp.include_router(chat.router, prefix="/api/chat", tags=["chat"])\napp.include_router(rag.router, prefix="/api/rag", tags=["rag"])\napp.include_router(content.router, prefix="/api/content", tags=["content"])\n\n@app.middleware("http")\nasync def add_process_time_header(request: Request, call_next):\n    """\n    Add processing time to response headers\n    """\n    start_time = time.time()\n    response = await call_next(request)\n    process_time = time.time() - start_time\n    response.headers["X-Process-Time"] = str(process_time)\n    return response\n\n@app.get("/")\nasync def root():\n    return {"message": "AI Documentation Backend API", "version": "1.0.0"}\n\n@app.get("/health")\nasync def health_check():\n    return {"status": "healthy", "timestamp": time.time()}\n\n@app.exception_handler(Exception)\nasync def global_exception_handler(request: Request, exc: Exception):\n    """\n    Global exception handler\n    """\n    logging.error(f"Unhandled exception: {str(exc)}", exc_info=True)\n    return JSONResponse(\n        status_code=500,\n        content={"detail": "Internal server error"}\n    )\n'})}),"\n",(0,s.jsx)(n.h4,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"backend/app/config.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from pydantic_settings import BaseSettings\nfrom typing import List, Optional\n\nclass Settings(BaseSettings):\n    # API Configuration\n    API_TITLE: str = "AI Documentation Backend"\n    API_VERSION: str = "1.0.0"\n\n    # Environment\n    ENVIRONMENT: str = "development"\n    DEBUG: bool = False\n\n    # OpenAI Configuration\n    OPENAI_API_KEY: str\n    OPENAI_MODEL: str = "gpt-3.5-turbo"\n    EMBEDDING_MODEL: str = "text-embedding-ada-002"\n\n    # Qdrant Configuration\n    QDRANT_URL: str\n    QDRANT_API_KEY: str\n    QDRANT_COLLECTION_NAME: str = "documentation"\n\n    # Database Configuration\n    QDRANT_GRPC_ENABLED: bool = True\n    QDRANT_TIMEOUT: int = 30\n\n    # Application Settings\n    ALLOWED_ORIGINS: List[str] = ["http://localhost:3000", "http://localhost:8080"]\n    CORS_ALLOW_CREDENTIALS: bool = True\n    CORS_ALLOW_METHODS: List[str] = ["*"]\n    CORS_ALLOW_HEADERS: List[str] = ["*"]\n\n    # Performance Settings\n    RAG_SEARCH_LIMIT: int = 5\n    RAG_SCORE_THRESHOLD: float = 0.3\n    MAX_TOKENS: int = 500\n    TEMPERATURE: float = 0.7\n\n    # Rate Limiting\n    RATE_LIMIT_REQUESTS: int = 100\n    RATE_LIMIT_WINDOW: int = 60  # seconds\n\n    class Config:\n        env_file = ".env"\n        case_sensitive = True\n\nsettings = Settings()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"data-models-and-schemas",children:"Data Models and Schemas"}),"\n",(0,s.jsx)(n.h4,{id:"request-and-response-models",children:"Request and Response Models"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"backend/app/schemas/chat.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from pydantic import BaseModel, Field\nfrom typing import List, Optional, Dict, Any\nfrom datetime import datetime\n\nclass ChatMessage(BaseModel):\n    role: str  # "user" or "assistant"\n    content: str\n    timestamp: Optional[datetime] = None\n\nclass ChatRequest(BaseModel):\n    message: str = Field(..., min_length=1, max_length=2000)\n    selected_text: Optional[str] = None\n    conversation_history: List[ChatMessage] = []\n    context_only: bool = False\n    temperature: float = Field(default=0.7, ge=0.0, le=1.0)\n    max_tokens: int = Field(default=500, ge=100, le=1000)\n\nclass ChatResponse(BaseModel):\n    response: str\n    sources: List[Dict[str, Any]]\n    query: str\n    conversation_id: Optional[str] = None\n    tokens_used: int = 0\n    processing_time: float = 0.0\n\nclass ChatStreamChunk(BaseModel):\n    content: str\n    type: str = "chunk"  # "start", "chunk", "end"\n    metadata: Optional[Dict[str, Any]] = None\n'})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"backend/app/schemas/rag.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from pydantic import BaseModel, Field\nfrom typing import List, Dict, Any, Optional\n\nclass SearchRequest(BaseModel):\n    query: str = Field(..., min_length=1, max_length=1000)\n    limit: int = Field(default=5, ge=1, le=20)\n    score_threshold: float = Field(default=0.3, ge=0.0, le=1.0)\n    filters: Optional[Dict[str, Any]] = None\n\nclass SearchResult(BaseModel):\n    id: str\n    content: str\n    metadata: Dict[str, Any]\n    score: float\n\nclass SearchResponse(BaseModel):\n    results: List[SearchResult]\n    query: str\n    processing_time: float\n\nclass RAGRequest(BaseModel):\n    query: str = Field(..., min_length=1, max_length=1000)\n    context_only: bool = False\n    selected_text: Optional[str] = None\n    max_results: int = Field(default=5, ge=1, le=20)\n    temperature: float = Field(default=0.7, ge=0.0, le=1.0)\n\nclass RAGResponse(BaseModel):\n    answer: str\n    sources: List[Dict[str, Any]]\n    query: str\n    confidence_score: float\n    processing_time: float\n"})}),"\n",(0,s.jsx)(n.h3,{id:"service-layer-implementation",children:"Service Layer Implementation"}),"\n",(0,s.jsx)(n.h4,{id:"rag-service",children:"RAG Service"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"backend/app/services/rag_service.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import logging\nfrom typing import List, Dict, Any, Optional, Tuple\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nimport openai\nimport tiktoken\nfrom app.config import settings\n\nlogger = logging.getLogger(__name__)\n\nclass RAGService:\n    def __init__(self):\n        self.qdrant_client = QdrantClient(\n            url=settings.QDRANT_URL,\n            api_key=settings.QDRANT_API_KEY,\n            prefer_grpc=settings.QDRANT_GRPC_ENABLED\n        )\n        openai.api_key = settings.OPENAI_API_KEY\n        self.encoder = tiktoken.encoding_for_model(settings.OPENAI_MODEL)\n\n    async def search_content(\n        self,\n        query: str,\n        limit: int = 5,\n        score_threshold: float = 0.3,\n        filters: Optional[Dict[str, Any]] = None\n    ) -> List[Dict[str, Any]]:\n        """\n        Search for relevant content in the vector database\n        """\n        try:\n            # Generate embedding for query\n            response = openai.Embedding.create(\n                input=query,\n                model=settings.EMBEDDING_MODEL\n            )\n            query_embedding = response[\'data\'][0][\'embedding\']\n\n            # Build search filters\n            search_filter = None\n            if filters:\n                filter_conditions = []\n                for key, value in filters.items():\n                    filter_conditions.append(\n                        models.FieldCondition(\n                            key=key,\n                            match=models.MatchValue(value=value)\n                        )\n                    )\n                if filter_conditions:\n                    search_filter = models.Filter(\n                        must=filter_conditions\n                    )\n\n            # Perform search in Qdrant\n            search_results = self.qdrant_client.search(\n                collection_name=settings.QDRANT_COLLECTION_NAME,\n                query_vector=query_embedding,\n                limit=limit,\n                score_threshold=score_threshold,\n                with_payload=True,\n                query_filter=search_filter\n            )\n\n            return [\n                {\n                    \'id\': str(result.id),\n                    \'content\': result.payload.get(\'content\', \'\'),\n                    \'metadata\': result.payload.get(\'metadata\', {}),\n                    \'score\': result.score\n                }\n                for result in search_results\n            ]\n\n        except Exception as e:\n            logger.error(f"Error in search_content: {str(e)}")\n            raise\n\n    async def generate_response(\n        self,\n        query: str,\n        context_chunks: List[Dict[str, Any]],\n        conversation_history: Optional[List[Dict[str, str]]] = None,\n        selected_text: Optional[str] = None,\n        temperature: float = 0.7\n    ) -> Tuple[str, Dict[str, Any]]:\n        """\n        Generate response using context and conversation history\n        """\n        try:\n            # Build context based on mode\n            if selected_text:\n                # Mode: Answer only from selected text\n                context = f"Answer only based on this text: {selected_text}"\n            else:\n                # Mode: Use retrieved context\n                context = self._assemble_context(context_chunks)\n\n            # Build system message\n            system_message = f"""\n            You are an AI documentation assistant.\n            Use the following context to answer the user\'s question: {context}\n\n            Provide helpful, accurate responses based on the provided information.\n            If the answer cannot be found in the provided context, acknowledge this.\n            """\n\n            # Build conversation messages\n            messages = [{"role": "system", "content": system_message}]\n\n            if conversation_history:\n                messages.extend(conversation_history)\n\n            messages.append({"role": "user", "content": query})\n\n            # Generate response\n            response = openai.ChatCompletion.create(\n                model=settings.OPENAI_MODEL,\n                messages=messages,\n                temperature=temperature,\n                max_tokens=settings.MAX_TOKENS\n            )\n\n            generated_text = response.choices[0].message.content\n            tokens_used = response.usage.total_tokens if response.usage else 0\n\n            return generated_text, {\n                \'tokens_used\': tokens_used,\n                \'model_used\': settings.OPENAI_MODEL,\n                \'confidence\': 0.8  # Placeholder for actual confidence calculation\n            }\n\n        except Exception as e:\n            logger.error(f"Error in generate_response: {str(e)}")\n            raise\n\n    def _assemble_context(self, chunks: List[Dict[str, Any]]) -> str:\n        """\n        Assemble context from retrieved chunks, respecting token limits\n        """\n        context_parts = []\n        total_tokens = 0\n        max_tokens = 3000  # Leave room for conversation and response\n\n        for chunk in chunks:\n            chunk_text = f"Source: {chunk.get(\'metadata\', {}).get(\'source\', \'Unknown\')}\\nContent: {chunk[\'content\']}\\n\\n"\n            chunk_tokens = len(self.encoder.encode(chunk_text))\n\n            if total_tokens + chunk_tokens > max_tokens:\n                break\n\n            context_parts.append(chunk_text)\n            total_tokens += chunk_tokens\n\n        return "".join(context_parts)\n\n    async def process_rag_query(\n        self,\n        query: str,\n        limit: int = 5,\n        score_threshold: float = 0.3,\n        selected_text: Optional[str] = None,\n        conversation_history: Optional[List[Dict[str, str]]] = None\n    ) -> Tuple[str, List[Dict[str, Any]], Dict[str, Any]]:\n        """\n        Complete RAG processing: search + generate response\n        """\n        import time\n        start_time = time.time()\n\n        # Search for relevant content (unless using selected text only)\n        if not selected_text:\n            context_chunks = await self.search_content(\n                query=query,\n                limit=limit,\n                score_threshold=score_threshold\n            )\n        else:\n            # For selected text mode, create a single chunk\n            context_chunks = [{\n                \'id\': \'selected_text\',\n                \'content\': selected_text,\n                \'metadata\': {\'source\': \'user_selected_text\'},\n                \'score\': 1.0\n            }]\n\n        # Generate response\n        response, metadata = await self.generate_response(\n            query=query,\n            context_chunks=context_chunks,\n            conversation_history=conversation_history,\n            selected_text=selected_text\n        )\n\n        processing_time = time.time() - start_time\n\n        return response, context_chunks, {\n            **metadata,\n            \'processing_time\': processing_time\n        }\n'})}),"\n",(0,s.jsx)(n.h4,{id:"content-service",children:"Content Service"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"backend/app/services/content_service.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import logging\nfrom typing import List, Dict, Any, Optional\nfrom app.config import settings\nimport openai\n\nlogger = logging.getLogger(__name__)\n\nclass ContentService:\n    def __init__(self):\n        openai.api_key = settings.OPENAI_API_KEY\n\n    async def generate_content(\n        self,\n        topic: str,\n        style: str = "educational",\n        length: str = "medium",\n        target_audience: str = "intermediate",\n        context: Optional[str] = None\n    ) -> Dict[str, Any]:\n        """\n        Generate educational content using AI\n        """\n        try:\n            prompt = self._build_content_prompt(\n                topic=topic,\n                style=style,\n                length=length,\n                target_audience=target_audience,\n                context=context\n            )\n\n            response = openai.ChatCompletion.create(\n                model=settings.OPENAI_MODEL,\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.7,\n                max_tokens=1000\n            )\n\n            content = response.choices[0].message.content\n\n            return {\n                \'content\': content,\n                \'metadata\': {\n                    \'topic\': topic,\n                    \'style\': style,\n                    \'target_audience\': target_audience,\n                    \'tokens_used\': response.usage.total_tokens if response.usage else 0\n                }\n            }\n\n        except Exception as e:\n            logger.error(f"Error in generate_content: {str(e)}")\n            raise\n\n    def _build_content_prompt(\n        self,\n        topic: str,\n        style: str,\n        length: str,\n        target_audience: str,\n        context: Optional[str] = None\n    ) -> str:\n        """\n        Build prompt for content generation\n        """\n        base_prompt = f"""\n        Write educational content about "{topic}" for {target_audience} level learners.\n        Write in a {style} style and make it {length} length.\n\n        Include these sections:\n        1. Hero Insight: [1-2 line emotional opening]\n        2. Purpose: [Clear statement of learning objective]\n        3. Deep Breakdown: [Main content with clear subsections]\n        4. UI/UX Angle: [Visual translation when needed]\n        5. Real-World Example: [Short, high-quality case]\n        6. Common Mistakes & Fixes: [Practical troubleshooting]\n        7. Micro-Exercises: [Small actionable tasks]\n        8. End-of-Section Summary: [5-7 bullet takeaways]\n        """\n\n        if context:\n            base_prompt = f"{base_prompt}\\n\\nContext: {context}"\n\n        return base_prompt\n'})}),"\n",(0,s.jsx)(n.h3,{id:"api-routes-implementation",children:"API Routes Implementation"}),"\n",(0,s.jsx)(n.h4,{id:"chat-routes",children:"Chat Routes"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"backend/app/routes/chat.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from fastapi import APIRouter, Depends, HTTPException\nfrom typing import List\nimport time\nimport logging\nfrom app.schemas.chat import ChatRequest, ChatResponse, ChatStreamChunk\nfrom app.schemas.rag import RAGRequest, RAGResponse\nfrom app.services.rag_service import RAGService\nfrom app.config import settings\n\nrouter = APIRouter()\nlogger = logging.getLogger(__name__)\n\n@router.post("/", response_model=ChatResponse)\nasync def chat_endpoint(request: ChatRequest):\n    """\n    Main chat endpoint that handles user queries\n    """\n    start_time = time.time()\n\n    try:\n        rag_service = RAGService()\n\n        response, sources, metadata = await rag_service.process_rag_query(\n            query=request.message,\n            selected_text=request.selected_text,\n            conversation_history=[\n                {\'role\': msg.role, \'content\': msg.content}\n                for msg in request.conversation_history\n            ]\n        )\n\n        processing_time = time.time() - start_time\n\n        return ChatResponse(\n            response=response,\n            sources=sources,\n            query=request.message,\n            tokens_used=metadata.get(\'tokens_used\', 0),\n            processing_time=processing_time\n        )\n\n    except Exception as e:\n        logger.error(f"Chat endpoint error: {str(e)}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.get("/stream")\nasync def chat_stream_endpoint():\n    """\n    Streaming chat endpoint for real-time responses\n    """\n    # Implementation for streaming responses would go here\n    # This would use Server-Sent Events (SSE) or WebSockets\n    pass\n'})}),"\n",(0,s.jsx)(n.h4,{id:"rag-routes",children:"RAG Routes"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"backend/app/routes/rag.py:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from fastapi import APIRouter, HTTPException\nimport time\nimport logging\nfrom app.schemas.rag import SearchRequest, SearchResponse, RAGRequest, RAGResponse\nfrom app.services.rag_service import RAGService\n\nrouter = APIRouter()\nlogger = logging.getLogger(__name__)\n\n@router.post("/search", response_model=SearchResponse)\nasync def search_endpoint(request: SearchRequest):\n    """\n    Semantic search endpoint\n    """\n    start_time = time.time()\n\n    try:\n        rag_service = RAGService()\n\n        results = await rag_service.search_content(\n            query=request.query,\n            limit=request.limit,\n            score_threshold=request.score_threshold,\n            filters=request.filters\n        )\n\n        processing_time = time.time() - start_time\n\n        return SearchResponse(\n            results=results,\n            query=request.query,\n            processing_time=processing_time\n        )\n\n    except Exception as e:\n        logger.error(f"Search endpoint error: {str(e)}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post("/answer", response_model=RAGResponse)\nasync def rag_answer_endpoint(request: RAGRequest):\n    """\n    Complete RAG answer generation endpoint\n    """\n    start_time = time.time()\n\n    try:\n        rag_service = RAGService()\n\n        response, sources, metadata = await rag_service.process_rag_query(\n            query=request.query,\n            limit=request.max_results,\n            selected_text=request.selected_text\n        )\n\n        processing_time = time.time() - start_time\n\n        return RAGResponse(\n            answer=response,\n            sources=[result[\'metadata\'] for result in sources],\n            query=request.query,\n            confidence_score=metadata.get(\'confidence\', 0.8),\n            processing_time=processing_time\n        )\n\n    except Exception as e:\n        logger.error(f"RAG answer endpoint error: {str(e)}")\n        raise HTTPException(status_code=500, detail=str(e))\n'})}),"\n",(0,s.jsx)(n.h3,{id:"performance-and-optimization",children:"Performance and Optimization"}),"\n",(0,s.jsx)(n.h4,{id:"caching-implementation",children:"Caching Implementation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import redis\nimport hashlib\nimport pickle\nfrom functools import wraps\n\nclass CacheService:\n    def __init__(self, redis_url: str = "redis://localhost:6379"):\n        self.redis_client = redis.from_url(redis_url)\n\n    def get_cache_key(self, prefix: str, *args, **kwargs) -> str:\n        """Generate cache key from function arguments"""\n        key_data = f"{prefix}:{str(args)}:{str(kwargs)}"\n        return f"cache:{hashlib.md5(key_data.encode()).hexdigest()}"\n\n    def cached(self, ttl: int = 300):\n        """Decorator for caching function results"""\n        def decorator(func):\n            @wraps(func)\n            async def wrapper(*args, **kwargs):\n                cache_key = self.get_cache_key(func.__name__, *args, **kwargs)\n\n                # Try to get from cache\n                cached_result = self.redis_client.get(cache_key)\n                if cached_result:\n                    return pickle.loads(cached_result)\n\n                # Execute function\n                result = await func(*args, **kwargs)\n\n                # Cache result\n                self.redis_client.setex(\n                    cache_key,\n                    ttl,\n                    pickle.dumps(result)\n                )\n\n                return result\n            return wrapper\n        return decorator\n\n# Usage in services\ncache_service = CacheService()\n\nclass RAGService:\n    # ... existing code ...\n\n    @cache_service.cached(ttl=600)  # Cache for 10 minutes\n    async def search_content(self, query: str, limit: int = 5, score_threshold: float = 0.3):\n        # ... existing search logic ...\n        pass\n'})}),"\n",(0,s.jsx)(n.h4,{id:"rate-limiting",children:"Rate Limiting"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from slowapi import Limiter, _rate_limit_exceeded_handler\nfrom slowapi.util import get_remote_address\nfrom fastapi import Request\n\nlimiter = Limiter(key_func=get_remote_address)\n\napp.state.limiter = limiter\napp.add_exception_handler(429, _rate_limit_exceeded_handler)\n\n@router.post("/", response_model=ChatResponse)\n@limiter.limit("10/minute")  # 10 requests per minute per IP\nasync def chat_endpoint(request: ChatRequest, request_obj: Request):\n    # ... existing implementation ...\n    pass\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-uiux-angle",children:"\ud83c\udfa8 UI/UX Angle"}),"\n",(0,s.jsx)(n.p,{children:"The backend performance directly impacts the user experience. Fast response times, proper error handling, and reliable service availability are crucial for maintaining a positive user experience with the AI assistant."}),"\n",(0,s.jsx)(n.h2,{id:"-real-world-example",children:"\ud83d\udcd8 Real-World Example"}),"\n",(0,s.jsx)(n.p,{children:"A production FastAPI backend handles:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"High-volume chat requests during peak usage"}),"\n",(0,s.jsx)(n.li,{children:"Complex RAG operations across large documentation sets"}),"\n",(0,s.jsx)(n.li,{children:"Proper error handling and fallback mechanisms"}),"\n",(0,s.jsx)(n.li,{children:"Performance monitoring and optimization"}),"\n",(0,s.jsx)(n.li,{children:"Secure API key management and rate limiting"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-common-mistakes--fixes",children:"\u26a0\ufe0f Common Mistakes & Fixes"}),"\n",(0,s.jsx)(n.h3,{id:"mistake-not-implementing-proper-error-handling-and-logging",children:"Mistake: Not implementing proper error handling and logging"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fix"}),": Add comprehensive exception handling and structured logging throughout the application."]}),"\n",(0,s.jsx)(n.h3,{id:"mistake-hardcoding-configuration-values-instead-of-using-environment-variables",children:"Mistake: Hardcoding configuration values instead of using environment variables"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fix"}),": Use Pydantic Settings for all configuration management."]}),"\n",(0,s.jsx)(n.h3,{id:"mistake-not-validating-input-data-properly",children:"Mistake: Not validating input data properly"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fix"}),": Use Pydantic models for comprehensive request validation."]}),"\n",(0,s.jsx)(n.h2,{id:"-micro-exercises",children:"\ud83e\uddea Micro-Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Exercise"}),": Set up the FastAPI project structure with proper routing and models."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Exercise"}),": Implement a basic RAG service with search and response generation."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"-reflection-prompts",children:"\ud83e\udded Reflection Prompts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"How does your current backend architecture handle AI service integration?"}),"\n",(0,s.jsx)(n.li,{children:"What challenges have you faced with API performance and reliability?"}),"\n",(0,s.jsx)(n.li,{children:"How might proper backend architecture improve your AI application?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"end-of-chapter-summary",children:"End-of-Chapter Summary"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"FastAPI provides a robust foundation for AI backend services"}),"\n",(0,s.jsx)(n.li,{children:"Proper architecture includes multiple service layers"}),"\n",(0,s.jsx)(n.li,{children:"Configuration management is crucial for deployment flexibility"}),"\n",(0,s.jsx)(n.li,{children:"Performance optimization ensures good user experience"}),"\n",(0,s.jsx)(n.li,{children:"The backend architecture enables scalable AI documentation systems"}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a});var s=t(6540);const r={},i=s.createContext(r);function a(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}}}]);