"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[132],{4811:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var t=i(4848),a=i(8453);const o={title:"Module 4 - AI-Robot Brain (NVIDIA Isaac)",sidebar_position:4,description:"NVIDIA Isaac platform capabilities for robotics perception and navigation",tags:["nvidia-isaac","perception","navigation","ai","robotics"]},s="Module 4: AI-Robot Brain (NVIDIA Isaac)",r={id:"module-4-ai-brain",title:"Module 4 - AI-Robot Brain (NVIDIA Isaac)",description:"NVIDIA Isaac platform capabilities for robotics perception and navigation",source:"@site/docs/module-4-ai-brain.mdx",sourceDirName:".",slug:"/module-4-ai-brain",permalink:"/book_writing_hackathon/docs/module-4-ai-brain",draft:!1,unlisted:!1,editUrl:"https://github.com/SANANAZ00/book_writing_hackathon/edit/main/docs/module-4-ai-brain.mdx",tags:[{label:"nvidia-isaac",permalink:"/book_writing_hackathon/docs/tags/nvidia-isaac"},{label:"perception",permalink:"/book_writing_hackathon/docs/tags/perception"},{label:"navigation",permalink:"/book_writing_hackathon/docs/tags/navigation"},{label:"ai",permalink:"/book_writing_hackathon/docs/tags/ai"},{label:"robotics",permalink:"/book_writing_hackathon/docs/tags/robotics"}],version:"current",sidebarPosition:4,frontMatter:{title:"Module 4 - AI-Robot Brain (NVIDIA Isaac)",sidebar_position:4,description:"NVIDIA Isaac platform capabilities for robotics perception and navigation",tags:["nvidia-isaac","perception","navigation","ai","robotics"]},sidebar:"tutorialSidebar",previous:{title:"Module 3 - Digital Twin (Gazebo & Unity)",permalink:"/book_writing_hackathon/docs/module-3-digital-twin"},next:{title:"Module 5 - Vision-Language-Action Integration",permalink:"/book_writing_hackathon/docs/module-5-vla"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"1. Introduction to NVIDIA Isaac Platform",id:"1-introduction-to-nvidia-isaac-platform",level:2},{value:"1.1 NVIDIA Isaac Architecture",id:"11-nvidia-isaac-architecture",level:3},{value:"1.2 Hardware Requirements",id:"12-hardware-requirements",level:3},{value:"2. Perception Model Training Techniques",id:"2-perception-model-training-techniques",level:2},{value:"2.1 Synthetic Data Generation",id:"21-synthetic-data-generation",level:3},{value:"2.2 Transfer Learning for Robotics",id:"22-transfer-learning-for-robotics",level:3},{value:"2.3 Multi-Modal Perception",id:"23-multi-modal-perception",level:3},{value:"3. VSLAM Implementation",id:"3-vslam-implementation",level:2},{value:"3.1 Understanding VSLAM for Humanoid Robots",id:"31-understanding-vslam-for-humanoid-robots",level:3},{value:"3.2 Isaac ROS VSLAM Components",id:"32-isaac-ros-vslam-components",level:3},{value:"3.3 Optimizing VSLAM for Humanoid Locomotion",id:"33-optimizing-vslam-for-humanoid-locomotion",level:3},{value:"4. Navigation System Development",id:"4-navigation-system-development",level:2},{value:"4.1 Isaac Navigation Stack",id:"41-isaac-navigation-stack",level:3},{value:"4.2 Navigation Pipeline Components",id:"42-navigation-pipeline-components",level:3},{value:"4.3 Human-Aware Navigation",id:"43-human-aware-navigation",level:3},{value:"5. Practical Exercise: Implementing Perception and Navigation",id:"5-practical-exercise-implementing-perception-and-navigation",level:2},{value:"Exercise 5.1: Object Detection Node",id:"exercise-51-object-detection-node",level:3},{value:"Exercise 5.2: Navigation Goal Planner",id:"exercise-52-navigation-goal-planner",level:3},{value:"6. Integration with ROS 2",id:"6-integration-with-ros-2",level:2},{value:"6.1 Isaac ROS Package Example",id:"61-isaac-ros-package-example",level:3},{value:"7. Summary",id:"7-summary",level:2},{value:"Assessment",id:"assessment",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"module-4-ai-robot-brain-nvidia-isaac",children:"Module 4: AI-Robot Brain (NVIDIA Isaac)"}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"After completing this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Explain NVIDIA Isaac platform capabilities for robotics"}),"\n",(0,t.jsx)(n.li,{children:"Demonstrate perception model training techniques"}),"\n",(0,t.jsx)(n.li,{children:"Show VSLAM (Visual Simultaneous Localization and Mapping) implementation"}),"\n",(0,t.jsx)(n.li,{children:"Include navigation system development guidance"}),"\n",(0,t.jsx)(n.li,{children:"Implement AI-driven perception and decision-making for humanoid robots"}),"\n",(0,t.jsx)(n.li,{children:"Integrate NVIDIA Isaac with ROS 2 for complete robotic systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completion of Module 1: Introduction"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Module 2: ROS 2 (Robotic Nervous System)"}),"\n",(0,t.jsx)(n.li,{children:"Completion of Module 3: Digital Twin (Gazebo & Unity)"}),"\n",(0,t.jsx)(n.li,{children:"Basic understanding of machine learning concepts"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with Python and neural networks"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"1-introduction-to-nvidia-isaac-platform",children:"1. Introduction to NVIDIA Isaac Platform"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac is a comprehensive platform for developing AI-powered robots, specifically designed to leverage NVIDIA's GPU computing capabilities for robotics applications. The platform provides a complete ecosystem for building, training, and deploying intelligent robotic systems with a focus on perception, navigation, and manipulation."}),"\n",(0,t.jsx)(n.p,{children:"For humanoid robotics, NVIDIA Isaac offers several key advantages:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"GPU-Accelerated Processing"}),": Optimized for real-time AI inference on NVIDIA hardware"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integrated Perception Pipeline"}),": Pre-built solutions for vision, localization, and mapping"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simulation Integration"}),": Seamless connection with Isaac Sim for training and testing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Compatibility"}),": Native integration with ROS 2 for robotics middleware"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Industrial-Grade Tools"}),": Production-ready components for real-world deployment"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"11-nvidia-isaac-architecture",children:"1.1 NVIDIA Isaac Architecture"}),"\n",(0,t.jsx)(n.p,{children:"The NVIDIA Isaac platform consists of several key components:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS"}),": ROS 2 packages optimized for GPU-accelerated perception"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac Sim"}),": High-fidelity simulation environment for training and testing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac Apps"}),": Pre-built applications for common robotics tasks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac SDK"}),": Software development kit for custom robot applications"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Deep Learning Tools"}),": Integration with NVIDIA's AI development ecosystem"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"12-hardware-requirements",children:"1.2 Hardware Requirements"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac is optimized for NVIDIA GPU hardware:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Jetson Platform"}),": For edge robotics applications (Jetson Nano, Xavier, Orin)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Desktop GPUs"}),": For development and simulation (RTX series recommended)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Center GPUs"}),": For large-scale training and complex inference"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The platform can also run on CPU-only systems, but performance will be significantly reduced."}),"\n",(0,t.jsx)(n.h2,{id:"2-perception-model-training-techniques",children:"2. Perception Model Training Techniques"}),"\n",(0,t.jsx)(n.p,{children:"Perception is the foundation of intelligent robotic behavior, enabling robots to understand and interact with their environment. NVIDIA Isaac provides comprehensive tools for training perception models specifically tailored for robotics applications."}),"\n",(0,t.jsx)(n.h3,{id:"21-synthetic-data-generation",children:"2.1 Synthetic Data Generation"}),"\n",(0,t.jsx)(n.p,{children:"One of Isaac's key strengths is its ability to generate synthetic training data using Isaac Sim. This approach addresses the challenge of collecting real-world data for training perception models:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Photorealistic Rendering"}),": Generate realistic images with perfect ground truth"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Variety of Conditions"}),": Simulate different lighting, weather, and environmental conditions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Scalability"}),": Generate thousands of training examples quickly and cost-effectively"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety"}),": Train on dangerous scenarios without real-world risk"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example: Using Isaac Sim for synthetic data generation\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.sensor import Camera\nimport numpy as np\n\nclass SyntheticDataGenerator:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n\n        # Add robot to the stage\n        add_reference_to_stage(\n            usd_path="/path/to/humanoid_robot.usd",\n            prim_path="/World/HumanoidRobot"\n        )\n\n        # Add camera sensor\n        self.camera = Camera(\n            prim_path="/World/HumanoidRobot/Camera",\n            frequency=30,\n            resolution=(640, 480)\n        )\n\n    def generate_training_data(self, num_samples=1000):\n        """Generate synthetic training data with ground truth labels"""\n        training_data = []\n\n        for i in range(num_samples):\n            # Randomize environment conditions\n            self.randomize_environment()\n\n            # Capture RGB, depth, and segmentation data\n            rgb_image = self.camera.get_rgb()\n            depth_image = self.camera.get_depth()\n            seg_image = self.camera.get_semantic_segmentation()\n\n            # Generate ground truth labels\n            labels = self.generate_ground_truth()\n\n            training_data.append({\n                \'rgb\': rgb_image,\n                \'depth\': depth_image,\n                \'segmentation\': seg_image,\n                \'labels\': labels\n            })\n\n            if i % 100 == 0:\n                print(f"Generated {i}/{num_samples} samples")\n\n        return training_data\n\n    def randomize_environment(self):\n        """Randomize lighting, objects, and environmental conditions"""\n        # Implementation details for randomization\n        pass\n\n    def generate_ground_truth(self):\n        """Generate ground truth labels for the current scene"""\n        # Implementation details for ground truth generation\n        return {}\n'})}),"\n",(0,t.jsx)(n.h3,{id:"22-transfer-learning-for-robotics",children:"2.2 Transfer Learning for Robotics"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac supports transfer learning approaches that adapt pre-trained models for specific robotics tasks:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example: Transfer learning for object detection in robotics\nimport torch\nimport torchvision\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.transforms import functional as F\n\nclass RoboticsObjectDetector:\n    def __init__(self, num_classes=10):\n        # Load pre-trained model\n        self.model = fasterrcnn_resnet50_fpn(pretrained=True)\n\n        # Replace the classifier with a new one for our specific classes\n        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n        self.model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n            in_features, num_classes + 1  # +1 for background\n        )\n\n        # Move to GPU if available\n        self.device = torch.device(\'cuda\') if torch.cuda.is_available() else torch.device(\'cpu\')\n        self.model.to(self.device)\n\n    def train(self, train_loader, num_epochs=10):\n        """Fine-tune the model on robotics-specific data"""\n        self.model.train()\n        optimizer = torch.optim.SGD(self.model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n\n        for epoch in range(num_epochs):\n            for images, targets in train_loader:\n                images = [image.to(self.device) for image in images]\n                targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n\n                loss_dict = self.model(images, targets)\n                losses = sum(loss for loss in loss_dict.values())\n\n                optimizer.zero_grad()\n                losses.backward()\n                optimizer.step()\n\n            lr_scheduler.step()\n            print(f"Epoch {epoch+1}/{num_epochs}, Loss: {losses.item():.4f}")\n\n    def predict(self, image):\n        """Run inference on a single image"""\n        self.model.eval()\n        with torch.no_grad():\n            prediction = self.model([image.to(self.device)])\n        return prediction\n'})}),"\n",(0,t.jsx)(n.h3,{id:"23-multi-modal-perception",children:"2.3 Multi-Modal Perception"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots require integration of multiple sensory modalities for comprehensive environmental understanding:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Visual Perception"}),": Object detection, recognition, and scene understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Depth Perception"}),": 3D scene reconstruction and spatial awareness"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Audio Perception"}),": Sound localization and speech recognition"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tactile Perception"}),": Force and touch sensing for manipulation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Proprioception"}),": Joint position and body awareness"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac provides tools to fuse these modalities effectively:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example: Multi-modal perception fusion\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass MultiModalPerceptor:\n    def __init__(self):\n        self.visual_processor = VisualProcessor()\n        self.audio_processor = AudioProcessor()\n        self.tactile_processor = TactileProcessor()\n        self.fusion_network = FusionNetwork()\n\n    def process_sensory_input(self, visual_data, audio_data, tactile_data, robot_state):\n        \"\"\"Process and fuse multi-modal sensory input\"\"\"\n        # Process individual modalities\n        visual_features = self.visual_processor.extract_features(visual_data)\n        audio_features = self.audio_processor.extract_features(audio_data)\n        tactile_features = self.tactile_processor.extract_features(tactile_data)\n\n        # Fuse features with robot state information\n        fused_features = self.fusion_network(\n            visual_features,\n            audio_features,\n            tactile_features,\n            robot_state\n        )\n\n        return fused_features\n\nclass FusionNetwork(torch.nn.Module):\n    def __init__(self, feature_dims):\n        super().__init__()\n        self.visual_fc = torch.nn.Linear(feature_dims['visual'], 256)\n        self.audio_fc = torch.nn.Linear(feature_dims['audio'], 128)\n        self.tactile_fc = torch.nn.Linear(feature_dims['tactile'], 64)\n        self.robot_state_fc = torch.nn.Linear(feature_dims['robot_state'], 128)\n\n        # Attention mechanism for dynamic fusion\n        self.attention = torch.nn.MultiheadAttention(568, num_heads=8)\n        self.output_layer = torch.nn.Linear(568, feature_dims['output'])\n\n    def forward(self, visual_features, audio_features, tactile_features, robot_state):\n        # Process individual features\n        v = torch.relu(self.visual_fc(visual_features))\n        a = torch.relu(self.audio_fc(audio_features))\n        t = torch.relu(self.tactile_fc(tactile_features))\n        r = torch.relu(self.robot_state_fc(robot_state))\n\n        # Concatenate features\n        combined = torch.cat([v, a, t, r], dim=-1)\n\n        # Apply attention-based fusion\n        attended, _ = self.attention(combined, combined, combined)\n\n        # Generate output\n        output = self.output_layer(attended)\n        return output\n"})}),"\n",(0,t.jsx)(n.h2,{id:"3-vslam-implementation",children:"3. VSLAM Implementation"}),"\n",(0,t.jsx)(n.p,{children:"Visual Simultaneous Localization and Mapping (VSLAM) is crucial for humanoid robots to navigate unknown environments. NVIDIA Isaac provides optimized VSLAM solutions that leverage GPU acceleration for real-time performance."}),"\n",(0,t.jsx)(n.h3,{id:"31-understanding-vslam-for-humanoid-robots",children:"3.1 Understanding VSLAM for Humanoid Robots"}),"\n",(0,t.jsx)(n.p,{children:"VSLAM enables humanoid robots to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Localize"})," themselves in unknown environments using visual input"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Map"})," the environment as they move through it"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Plan paths"})," through complex, dynamic environments"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Avoid obstacles"})," in real-time"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"For humanoid robots, VSLAM must handle additional challenges:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Dynamic motion"}),": Walking creates complex camera motion patterns"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Height variation"}),": Head movement during walking affects visual input"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Social navigation"}),": Navigating around humans requires special considerations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time constraints"}),": Must operate within strict timing requirements"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"32-isaac-ros-vslam-components",children:"3.2 Isaac ROS VSLAM Components"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac provides several ROS 2 packages for VSLAM:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Visual SLAM"}),": GPU-accelerated visual SLAM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Stereo Image Proc"}),": Stereo processing for depth estimation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Apriltag"}),": Marker-based localization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Object Detection"}),": Real-time object detection and tracking"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example: Isaac ROS Visual SLAM node\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import Header\n\nclass IsaacVSLAMNode(Node):\n    def __init__(self):\n        super().__init__(\'isaac_vslam_node\')\n\n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/camera/rgb/image_rect_color\',\n            self.image_callback,\n            10\n        )\n\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo,\n            \'/camera/rgb/camera_info\',\n            self.camera_info_callback,\n            10\n        )\n\n        # Publishers\n        self.odom_pub = self.create_publisher(Odometry, \'/visual_odometry\', 10)\n        self.map_pub = self.create_publisher(OccupancyGrid, \'/visual_map\', 10)\n\n        # VSLAM components\n        self.vslam_pipeline = self.initialize_vslam_pipeline()\n        self.camera_intrinsics = None\n        self.poses = []\n\n    def initialize_vslam_pipeline(self):\n        """Initialize the VSLAM pipeline with GPU acceleration"""\n        # This would typically interface with Isaac ROS Visual SLAM\n        # components that leverage NVIDIA GPUs for processing\n        pass\n\n    def image_callback(self, msg):\n        """Process incoming image for VSLAM"""\n        if self.camera_intrinsics is None:\n            return\n\n        # Convert ROS image to format expected by VSLAM pipeline\n        image = self.ros_image_to_cv2(msg)\n\n        # Process image through VSLAM pipeline\n        pose, map_update = self.process_vslam_frame(image, self.camera_intrinsics)\n\n        if pose is not None:\n            self.poses.append(pose)\n            self.publish_odometry(pose)\n\n        if map_update is not None:\n            self.publish_map(map_update)\n\n    def process_vslam_frame(self, image, camera_intrinsics):\n        """Process a single frame through the VSLAM pipeline"""\n        # Implementation would use Isaac ROS Visual SLAM components\n        # to perform feature extraction, tracking, and mapping\n        pass\n\n    def publish_odometry(self, pose):\n        """Publish odometry information"""\n        odom_msg = Odometry()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = \'map\'\n        odom_msg.child_frame_id = \'base_link\'\n\n        # Set pose (simplified)\n        odom_msg.pose.pose.position.x = pose[0]\n        odom_msg.pose.pose.position.y = pose[1]\n        odom_msg.pose.pose.position.z = pose[2]\n\n        # Set orientation (simplified)\n        odom_msg.pose.pose.orientation.w = 1.0  # Identity quaternion\n\n        self.odom_pub.publish(odom_msg)\n\n    def publish_map(self, map_data):\n        """Publish occupancy grid map"""\n        # Implementation would publish occupancy grid\n        pass\n'})}),"\n",(0,t.jsx)(n.h3,{id:"33-optimizing-vslam-for-humanoid-locomotion",children:"3.3 Optimizing VSLAM for Humanoid Locomotion"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots present unique challenges for VSLAM due to their dynamic motion patterns:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example: VSLAM optimization for humanoid walking\nclass HumanoidVSLAMOptimizer:\n    def __init__(self):\n        self.step_detector = StepDetector()\n        self.motion_compensator = MotionCompensator()\n        self.keyframe_selector = KeyframeSelector()\n\n    def process_humanoid_vslam(self, image, imu_data, joint_positions):\n        """Optimize VSLAM for humanoid robot motion"""\n        # Detect walking steps to identify stable periods\n        step_info = self.step_detector.detect_step(joint_positions)\n\n        # Compensate for walking motion\n        compensated_image = self.motion_compensator.compensate(\n            image, imu_data, joint_positions\n        )\n\n        # Select keyframes during stable walking phases\n        is_keyframe = self.keyframe_selector.should_select_keyframe(\n            step_info, compensated_image\n        )\n\n        if is_keyframe:\n            # Process as normal VSLAM frame\n            return self.process_vslam_frame(compensated_image)\n        else:\n            # Use motion models for prediction during unstable periods\n            return self.predict_pose_with_motion_model(imu_data)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"4-navigation-system-development",children:"4. Navigation System Development"}),"\n",(0,t.jsx)(n.p,{children:"Navigation is the process of planning and executing paths through environments. For humanoid robots, navigation must consider human-aware planning, dynamic obstacle avoidance, and safe interaction with humans."}),"\n",(0,t.jsx)(n.h3,{id:"41-isaac-navigation-stack",children:"4.1 Isaac Navigation Stack"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac provides a comprehensive navigation stack built on ROS 2 Navigation2:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Nav2"}),": Standard ROS 2 navigation framework"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Navigation"}),": GPU-accelerated navigation components"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Behavior Trees"}),": Flexible task planning and execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Human-Aware Navigation"}),": Specialized planning for human environments"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"42-navigation-pipeline-components",children:"4.2 Navigation Pipeline Components"}),"\n",(0,t.jsx)(n.p,{children:"The navigation pipeline consists of several key components:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Global Planner"}),": Plans long-term paths through known maps"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Local Planner"}),": Plans short-term trajectories avoiding immediate obstacles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Controller"}),": Converts planned trajectories into motor commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Recovery Behaviors"}),": Handles navigation failures and stuck situations"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example: Navigation system implementation\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionClient\nimport math\n\nclass HumanoidNavigationSystem(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_navigation_system\')\n\n        # Action client for navigation\n        self.nav_client = ActionClient(\n            self,\n            NavigateToPose,\n            \'navigate_to_pose\'\n        )\n\n        # Publisher for goal poses\n        self.goal_pub = self.create_publisher(\n            PoseStamped,\n            \'/goal_pose\',\n            10\n        )\n\n        # Subscriptions for safety\n        self.human_detector_sub = self.create_subscription(\n            HumanDetectionArray,\n            \'/human_detector/detections\',\n            self.human_detection_callback,\n            10\n        )\n\n        self.current_goal = None\n        self.safety_enabled = True\n\n    def navigate_to_pose(self, x, y, theta):\n        """Navigate to a specific pose with human-aware safety"""\n        # Wait for action server\n        self.nav_client.wait_for_server()\n\n        # Create goal message\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = \'map\'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n        goal_msg.pose.pose.position.x = x\n        goal_msg.pose.pose.position.y = y\n        goal_msg.pose.pose.position.z = 0.0\n\n        # Convert theta to quaternion\n        from tf_transformations import quaternion_from_euler\n        quat = quaternion_from_euler(0, 0, theta)\n        goal_msg.pose.pose.orientation.x = quat[0]\n        goal_msg.pose.pose.orientation.y = quat[1]\n        goal_msg.pose.pose.orientation.z = quat[2]\n        goal_msg.pose.pose.orientation.w = quat[3]\n\n        # Send goal\n        self.current_goal = goal_msg\n        self._send_goal_future = self.nav_client.send_goal_async(\n            goal_msg,\n            feedback_callback=self.feedback_callback\n        )\n\n        self._send_goal_future.add_done_callback(self.goal_response_callback)\n\n    def goal_response_callback(self, future):\n        """Handle goal response"""\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info(\'Goal rejected :(\')\n            return\n\n        self.get_logger().info(\'Goal accepted :)\')\n        self._get_result_future = goal_handle.get_result_async()\n        self._get_result_future.add_done_callback(self.get_result_callback)\n\n    def get_result_callback(self, future):\n        """Handle navigation result"""\n        result = future.result().result\n        self.get_logger().info(f\'Result: {result}\')\n\n    def feedback_callback(self, feedback_msg):\n        """Handle navigation feedback"""\n        feedback = feedback_msg.feedback\n        self.get_logger().info(f\'Navigating... {feedback}\')\n\n    def human_detection_callback(self, msg):\n        """Handle human detection for safety"""\n        if not self.safety_enabled:\n            return\n\n        # Check if humans are in the navigation path\n        for detection in msg.detections:\n            distance = self.calculate_distance_to_robot(detection.pose)\n            if distance < 1.0:  # Less than 1 meter\n                self.get_logger().warn(\'Human detected in path, pausing navigation\')\n                # Implement safety behavior (stop, wait, replan)\n                self.pause_navigation()\n\n    def pause_navigation(self):\n        """Pause current navigation for safety"""\n        # Implementation would send cancel command to navigation server\n        pass\n\n    def calculate_distance_to_robot(self, pose):\n        """Calculate distance from detected object to robot"""\n        # Implementation would get robot position and calculate distance\n        return 0.0  # Simplified\n'})}),"\n",(0,t.jsx)(n.h3,{id:"43-human-aware-navigation",children:"4.3 Human-Aware Navigation"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots must navigate safely around humans, which requires specialized planning algorithms:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Social Force Model"}),": Models human movement patterns and social interactions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Personal Space Respect"}),": Maintains appropriate distances from humans"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Predictive Path Planning"}),": Anticipates human movements"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Non-Verbal Communication"}),": Uses robot motion to signal intentions"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example: Human-aware navigation costmap\nclass HumanAwareCostmap:\n    def __init__(self):\n        self.base_costmap = None  # Standard costmap\n        self.human_influence_radius = 1.5  # meters\n        self.social_force_coefficient = 0.8\n\n    def update_with_humans(self, human_poses, robot_pose):\n        """Update costmap with human positions"""\n        # Copy base costmap\n        human_aware_costmap = self.base_costmap.copy()\n\n        # Add influence of humans\n        for human_pose in human_poses:\n            influence = self.calculate_human_influence(\n                human_pose, robot_pose\n            )\n            human_aware_costmap = self.apply_influence(\n                human_aware_costmap, human_pose, influence\n            )\n\n        return human_aware_costmap\n\n    def calculate_human_influence(self, human_pose, robot_pose):\n        """Calculate social influence of human on robot path"""\n        distance = self.euclidean_distance(human_pose, robot_pose)\n\n        if distance < self.human_influence_radius:\n            # Higher cost closer to humans\n            influence = self.social_force_coefficient * (\n                1 - distance / self.human_influence_radius\n            )\n            return influence\n        else:\n            return 0.0\n\n    def euclidean_distance(self, pose1, pose2):\n        """Calculate Euclidean distance between two poses"""\n        dx = pose1.position.x - pose2.position.x\n        dy = pose1.position.y - pose2.position.y\n        return math.sqrt(dx*dx + dy*dy)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"5-practical-exercise-implementing-perception-and-navigation",children:"5. Practical Exercise: Implementing Perception and Navigation"}),"\n",(0,t.jsx)(n.p,{children:"Let's implement a complete perception and navigation system for a humanoid robot using NVIDIA Isaac components."}),"\n",(0,t.jsx)(n.h3,{id:"exercise-51-object-detection-node",children:"Exercise 5.1: Object Detection Node"}),"\n",(0,t.jsx)(n.p,{children:"Create a node that uses Isaac's optimized perception for object detection:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# object_detector.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom geometry_msgs.msg import Point\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass IsaacObjectDetector(Node):\n    def __init__(self):\n        super().__init__('isaac_object_detector')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Create subscription to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/camera/rgb/image_rect_color',\n            self.image_callback,\n            10\n        )\n\n        # Create publisher for detections\n        self.detection_pub = self.create_publisher(\n            Detection2DArray,\n            '/object_detector/detections',\n            10\n        )\n\n        # Initialize Isaac-optimized detection model\n        # (In practice, this would use Isaac ROS detection components)\n        self.detection_model = self.initialize_detection_model()\n\n        self.get_logger().info('Isaac Object Detector initialized')\n\n    def initialize_detection_model(self):\n        \"\"\"Initialize Isaac-optimized detection model\"\"\"\n        # This would typically load a TensorRT optimized model\n        # from Isaac ROS object detection package\n        pass\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image and detect objects\"\"\"\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n\n            # Perform object detection (simplified)\n            detections = self.detect_objects(cv_image)\n\n            # Publish detections in vision_msgs format\n            detection_msg = self.create_detection_message(detections, msg.header)\n            self.detection_pub.publish(detection_msg)\n\n            self.get_logger().info(f'Detected {len(detections)} objects')\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing image: {e}')\n\n    def detect_objects(self, image):\n        \"\"\"Perform object detection on image\"\"\"\n        # In practice, this would use Isaac ROS optimized detection\n        # For this example, we'll simulate detections\n        simulated_detections = [\n            {\n                'class': 'person',\n                'confidence': 0.95,\n                'bbox': [100, 100, 200, 200],  # [x, y, width, height]\n                'center': [200, 200]\n            },\n            {\n                'class': 'chair',\n                'confidence': 0.87,\n                'bbox': [300, 150, 150, 150],\n                'center': [375, 225]\n            }\n        ]\n        return simulated_detections\n\n    def create_detection_message(self, detections, header):\n        \"\"\"Create vision_msgs Detection2DArray from detections\"\"\"\n        detection_array = Detection2DArray()\n        detection_array.header = header\n\n        for detection in detections:\n            detection_2d = Detection2D()\n            detection_2d.header = header\n\n            # Set bounding box\n            detection_2d.bbox.center.x = detection['center'][0]\n            detection_2d.bbox.center.y = detection['center'][1]\n            detection_2d.bbox.size_x = detection['bbox'][2]\n            detection_2d.bbox.size_y = detection['bbox'][3]\n\n            # Set results (classification)\n            result = ObjectHypothesisWithPose()\n            result.hypothesis.class_id = detection['class']\n            result.hypothesis.score = detection['confidence']\n            detection_2d.results.append(result)\n\n            detection_array.detections.append(detection_2d)\n\n        return detection_array\n\ndef main(args=None):\n    rclpy.init(args=args)\n    detector = IsaacObjectDetector()\n    rclpy.spin(detector)\n    detector.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"exercise-52-navigation-goal-planner",children:"Exercise 5.2: Navigation Goal Planner"}),"\n",(0,t.jsx)(n.p,{children:"Create a node that plans navigation goals based on detected objects:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# goal_planner.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped\nfrom vision_msgs.msg import Detection2DArray\nfrom std_msgs.msg import String\nimport math\n\nclass NavigationGoalPlanner(Node):\n    def __init__(self):\n        super().__init__('navigation_goal_planner')\n\n        # Subscription to object detections\n        self.detection_sub = self.create_subscription(\n            Detection2DArray,\n            '/object_detector/detections',\n            self.detection_callback,\n            10\n        )\n\n        # Publisher for navigation goals\n        self.goal_pub = self.create_publisher(\n            PoseStamped,\n            '/goal_planner/goal',\n            10\n        )\n\n        # Publisher for robot behavior state\n        self.state_pub = self.create_publisher(\n            String,\n            '/robot_state',\n            10\n        )\n\n        self.robot_position = [0.0, 0.0]  # Current robot position\n        self.target_object = None\n        self.get_logger().info('Navigation Goal Planner initialized')\n\n    def detection_callback(self, msg):\n        \"\"\"Process object detections and plan navigation goals\"\"\"\n        if not msg.detections:\n            return\n\n        # Find the most interesting object to approach\n        target_detection = self.select_target_object(msg.detections)\n\n        if target_detection:\n            # Calculate goal position near the target object\n            goal_pose = self.calculate_approach_pose(target_detection)\n\n            # Publish navigation goal\n            goal_msg = PoseStamped()\n            goal_msg.header.stamp = self.get_clock().now().to_msg()\n            goal_msg.header.frame_id = 'map'\n            goal_msg.pose = goal_pose\n\n            self.goal_pub.publish(goal_msg)\n            self.get_logger().info(f'Published goal to approach {target_detection.results[0].hypothesis.class_id}')\n\n            # Publish robot state\n            state_msg = String()\n            state_msg.data = f'approaching_{target_detection.results[0].hypothesis.class_id}'\n            self.state_pub.publish(state_msg)\n\n    def select_target_object(self, detections):\n        \"\"\"Select the most interesting object to approach\"\"\"\n        # For this example, approach the closest person\n        closest_person = None\n        min_distance = float('inf')\n\n        for detection in detections:\n            if detection.results and detection.results[0].hypothesis.class_id == 'person':\n                # Calculate distance to robot (simplified)\n                distance = self.estimate_distance_to_robot(detection)\n\n                if distance < min_distance:\n                    min_distance = distance\n                    closest_person = detection\n\n        return closest_person\n\n    def estimate_distance_to_robot(self, detection):\n        \"\"\"Estimate distance from detection to robot\"\"\"\n        # In practice, this would use depth information\n        # For this example, we'll use a simplified approach\n        return 2.0  # Fixed distance for simulation\n\n    def calculate_approach_pose(self, detection):\n        \"\"\"Calculate approach pose for the target object\"\"\"\n        # This would use actual coordinates in a real system\n        # For this example, we'll create a pose 1 meter away\n        from geometry_msgs.msg import Pose\n        approach_pose = Pose()\n        approach_pose.position.x = 1.0  # 1 meter in front\n        approach_pose.position.y = 0.0\n        approach_pose.position.z = 0.0\n\n        # Face the object (simplified orientation)\n        from tf_transformations import quaternion_from_euler\n        quat = quaternion_from_euler(0, 0, 0)\n        approach_pose.orientation.x = quat[0]\n        approach_pose.orientation.y = quat[1]\n        approach_pose.orientation.z = quat[2]\n        approach_pose.orientation.w = quat[3]\n\n        return approach_pose\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = NavigationGoalPlanner()\n    rclpy.spin(planner)\n    planner.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h2,{id:"6-integration-with-ros-2",children:"6. Integration with ROS 2"}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac seamlessly integrates with ROS 2, allowing you to leverage both ecosystems:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Isaac ROS Packages"}),": GPU-accelerated ROS 2 nodes for perception and navigation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Standard ROS 2 Nodes"}),": Traditional ROS 2 components for control and coordination"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Message Compatibility"}),": Isaac components use standard ROS 2 message types"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Launch System Integration"}),": Use ROS 2 launch files for Isaac applications"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"61-isaac-ros-package-example",children:"6.1 Isaac ROS Package Example"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"# Example: ROS 2 launch file for Isaac perception stack\n# launch/isaac_perception.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\n\ndef generate_launch_description():\n    # Declare launch arguments\n    use_sim_time = LaunchConfiguration('use_sim_time', default='false')\n\n    return LaunchDescription([\n        # Isaac ROS Visual SLAM node\n        Node(\n            package='isaac_ros_visual_slam',\n            executable='visual_slam_node',\n            name='visual_slam',\n            parameters=[{\n                'use_sim_time': use_sim_time,\n                'enable_occupancy_map': True,\n                'occupancy_map_depth': 20,\n            }],\n            remappings=[\n                ('/visual_slam/image', '/camera/rgb/image_rect_color'),\n                ('/visual_slam/camera_info', '/camera/rgb/camera_info'),\n            ]\n        ),\n\n        # Isaac ROS AprilTag node\n        Node(\n            package='isaac_ros_apriltag',\n            executable='apriltag_node',\n            name='apriltag',\n            parameters=[{\n                'use_sim_time': use_sim_time,\n                'family': 'tag36h11',\n                'max_tags': 64,\n                'tag36h11_size': 0.166,  # tag size in meters\n            }],\n            remappings=[\n                ('/image', '/camera/rgb/image_rect_color'),\n                ('/camera_info', '/camera/rgb/camera_info'),\n            ]\n        ),\n\n        # Isaac ROS Object Detection\n        Node(\n            package='isaac_ros_detectnet',\n            executable='detectnet_node',\n            name='detectnet',\n            parameters=[{\n                'use_sim_time': use_sim_time,\n                'model_name': 'ssd_mobilenet_v2_coco',\n                'input_tensor': 'input_tensor',\n                'output_layer_names': ['scores', 'boxes', 'classes'],\n            }],\n            remappings=[\n                ('/image', '/camera/rgb/image_rect_color'),\n            ]\n        )\n    ])\n"})}),"\n",(0,t.jsx)(n.h2,{id:"7-summary",children:"7. Summary"}),"\n",(0,t.jsx)(n.p,{children:"In this module, you've learned about the NVIDIA Isaac platform and its capabilities for creating intelligent robotic systems. You now understand:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"How to leverage NVIDIA Isaac for GPU-accelerated perception and navigation"}),"\n",(0,t.jsx)(n.li,{children:"The techniques for training perception models using synthetic data"}),"\n",(0,t.jsx)(n.li,{children:"How to implement VSLAM systems optimized for humanoid robot motion"}),"\n",(0,t.jsx)(n.li,{children:"The components of a comprehensive navigation system for humanoid robots"}),"\n",(0,t.jsx)(n.li,{children:"How to integrate Isaac components with the broader ROS 2 ecosystem"}),"\n",(0,t.jsx)(n.li,{children:"Best practices for developing AI-driven robotic behaviors"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"NVIDIA Isaac provides powerful tools for creating intelligent humanoid robots capable of perceiving, understanding, and navigating complex environments. The platform's focus on GPU acceleration makes it particularly suitable for real-time AI applications in robotics."}),"\n",(0,t.jsx)(n.p,{children:"In the next module, we'll explore how to integrate vision, language, and action systems to create comprehensive humanoid robot capabilities that can interact naturally with humans and environments."}),"\n",(0,t.jsx)(n.h2,{id:"assessment",children:"Assessment"}),"\n",(0,t.jsx)(n.p,{children:"Complete the following exercises to reinforce your understanding:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Implement a complete perception pipeline using Isaac ROS components"}),"\n",(0,t.jsx)(n.li,{children:"Create a VSLAM system that works robustly during humanoid walking"}),"\n",(0,t.jsx)(n.li,{children:"Develop a human-aware navigation system that respects personal space"}),"\n",(0,t.jsx)(n.li,{children:"Train a perception model using synthetic data generated in Isaac Sim"}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s});var t=i(6540);const a={},o=t.createContext(a);function s(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}}}]);