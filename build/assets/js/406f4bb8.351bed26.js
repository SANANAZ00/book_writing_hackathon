"use strict";(globalThis.webpackChunkphysical_ai_robotics=globalThis.webpackChunkphysical_ai_robotics||[]).push([[447],{3466:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>a,toc:()=>l});var s=t(4848),i=t(8453);const r={sidebar_position:3},o="Building the RAG Chatbot Foundation",a={id:"systems/building-rag-chatbot",title:"Building the RAG Chatbot Foundation",description:"Hero Insight",source:"@site/docs/systems/building-rag-chatbot.mdx",sourceDirName:"systems",slug:"/systems/building-rag-chatbot",permalink:"/book_writing_hackathon/docs/systems/building-rag-chatbot",draft:!1,unlisted:!1,editUrl:"https://github.com/SANANAZ00/book_writing_hackathon/edit/main/docs/systems/building-rag-chatbot.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3}},c={},l=[{value:"Hero Insight",id:"hero-insight",level:2},{value:"Purpose of This Chapter",id:"purpose-of-this-chapter",level:2},{value:"Deep Breakdown",id:"deep-breakdown",level:2},{value:"RAG Chatbot Architecture",id:"rag-chatbot-architecture",level:3},{value:"1. Frontend Interface",id:"1-frontend-interface",level:4},{value:"2. Backend API Layer",id:"2-backend-api-layer",level:4},{value:"3. RAG Pipeline",id:"3-rag-pipeline",level:4},{value:"Implementing the RAG Backend",id:"implementing-the-rag-backend",level:3},{value:"Core RAG Service",id:"core-rag-service",level:4},{value:"FastAPI RAG Endpoints",id:"fastapi-rag-endpoints",level:4},{value:"Frontend Chat Widget Implementation",id:"frontend-chat-widget-implementation",level:3},{value:"React Chat Widget Component",id:"react-chat-widget-component",level:4},{value:"Advanced RAG Features",id:"advanced-rag-features",level:3},{value:"Selected Text Only Mode",id:"selected-text-only-mode",level:4},{value:"Context-Aware Response Generation",id:"context-aware-response-generation",level:4},{value:"Performance Optimization",id:"performance-optimization",level:3},{value:"Caching Strategies",id:"caching-strategies",level:4},{value:"\ud83c\udfa8 UI/UX Angle",id:"-uiux-angle",level:2},{value:"\ud83d\udcd8 Real-World Example",id:"-real-world-example",level:2},{value:"\u26a0\ufe0f Common Mistakes &amp; Fixes",id:"\ufe0f-common-mistakes--fixes",level:2},{value:"Mistake: Not handling token limits properly in context assembly",id:"mistake-not-handling-token-limits-properly-in-context-assembly",level:3},{value:"Mistake: Providing responses without source attribution",id:"mistake-providing-responses-without-source-attribution",level:3},{value:"Mistake: Not differentiating between selected-text and full-document responses",id:"mistake-not-differentiating-between-selected-text-and-full-document-responses",level:3},{value:"\ud83e\uddea Micro-Exercises",id:"-micro-exercises",level:2},{value:"\ud83e\udded Reflection Prompts",id:"-reflection-prompts",level:2},{value:"End-of-Chapter Summary",id:"end-of-chapter-summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"building-the-rag-chatbot-foundation",children:"Building the RAG Chatbot Foundation"}),"\n",(0,s.jsx)(n.h2,{id:"hero-insight",children:"Hero Insight"}),"\n",(0,s.jsx)(n.p,{children:"Transform static documentation into an intelligent, conversational assistant that understands and responds to complex queries with contextual precision."}),"\n",(0,s.jsx)(n.h2,{id:"purpose-of-this-chapter",children:"Purpose of This Chapter"}),"\n",(0,s.jsx)(n.p,{children:"You will build the foundational RAG (Retrieval-Augmented Generation) chatbot system that can answer questions about your documentation content with accuracy and contextual awareness."}),"\n",(0,s.jsx)(n.h2,{id:"deep-breakdown",children:"Deep Breakdown"}),"\n",(0,s.jsx)(n.h3,{id:"rag-chatbot-architecture",children:"RAG Chatbot Architecture"}),"\n",(0,s.jsx)(n.p,{children:"A complete RAG chatbot system consists of several interconnected components:"}),"\n",(0,s.jsx)(n.h4,{id:"1-frontend-interface",children:"1. Frontend Interface"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Chat Widget"}),": Floating or integrated chat interface"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Message Display"}),": Proper formatting for user and AI messages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Input Handling"}),": Text input with send functionality"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Loading States"}),": Visual feedback during processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Citation Display"}),": Showing sources for AI responses"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"2-backend-api-layer",children:"2. Backend API Layer"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Message Processing"}),": Handling user queries and context"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Content Retrieval"}),": Fetching relevant documentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response Generation"}),": Creating contextual responses"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Conversation Management"}),": Maintaining dialogue history"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Error Handling"}),": Graceful degradation for failures"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"3-rag-pipeline",children:"3. RAG Pipeline"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Query Processing"}),": Understanding user questions"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vector Search"}),": Finding relevant documentation chunks"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Context Assembly"}),": Combining retrieved content"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Response Generation"}),": Creating final answers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Source Attribution"}),": Providing citations"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"implementing-the-rag-backend",children:"Implementing the RAG Backend"}),"\n",(0,s.jsx)(n.h4,{id:"core-rag-service",children:"Core RAG Service"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import openai\nfrom typing import List, Dict, Optional, Tuple\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http import models\nimport tiktoken\n\nclass RAGService:\n    def __init__(self, qdrant_url: str, qdrant_api_key: str, openai_api_key: str):\n        self.qdrant_client = QdrantClient(\n            url=qdrant_url,\n            api_key=qdrant_api_key,\n            prefer_grpc=True\n        )\n        openai.api_key = openai_api_key\n        self.encoder = tiktoken.encoding_for_model("gpt-3.5-turbo")\n\n    def retrieve_relevant_content(\n        self,\n        query: str,\n        limit: int = 5,\n        score_threshold: float = 0.3\n    ) -> List[Dict]:\n        """\n        Retrieve relevant content from vector database\n        """\n        # Generate embedding for query\n        response = openai.Embedding.create(\n            input=query,\n            model="text-embedding-ada-002"\n        )\n        query_embedding = response[\'data\'][0][\'embedding\']\n\n        # Search in Qdrant\n        search_results = self.qdrant_client.search(\n            collection_name="documentation",\n            query_vector=query_embedding,\n            limit=limit,\n            score_threshold=score_threshold,\n            with_payload=True\n        )\n\n        return [\n            {\n                \'content\': result.payload[\'content\'],\n                \'metadata\': result.payload.get(\'metadata\', {}),\n                \'score\': result.score\n            }\n            for result in search_results\n        ]\n\n    def generate_response(\n        self,\n        query: str,\n        context_chunks: List[Dict],\n        conversation_history: Optional[List[Dict]] = None\n    ) -> str:\n        """\n        Generate contextual response using retrieved content\n        """\n        # Limit context to fit within token limits\n        context = self._assemble_context(context_chunks)\n\n        # Build conversation messages\n        messages = [\n            {\n                "role": "system",\n                "content": f"You are an AI assistant for documentation. Use the following context to answer questions: {context}"\n            }\n        ]\n\n        if conversation_history:\n            messages.extend(conversation_history)\n\n        messages.append({"role": "user", "content": query})\n\n        response = openai.ChatCompletion.create(\n            model="gpt-3.5-turbo",\n            messages=messages,\n            temperature=0.7,\n            max_tokens=500\n        )\n\n        return response.choices[0].message.content\n\n    def _assemble_context(self, chunks: List[Dict]) -> str:\n        """\n        Assemble context from retrieved chunks, respecting token limits\n        """\n        context_parts = []\n        total_tokens = 0\n        max_tokens = 3000  # Leave room for conversation and response\n\n        for chunk in chunks:\n            chunk_text = f"Source: {chunk.get(\'metadata\', {}).get(\'source\', \'Unknown\')}\\nContent: {chunk[\'content\']}\\n\\n"\n            chunk_tokens = len(self.encoder.encode(chunk_text))\n\n            if total_tokens + chunk_tokens > max_tokens:\n                break\n\n            context_parts.append(chunk_text)\n            total_tokens += chunk_tokens\n\n        return "".join(context_parts)\n'})}),"\n",(0,s.jsx)(n.h4,{id:"fastapi-rag-endpoints",children:"FastAPI RAG Endpoints"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from fastapi import APIRouter, HTTPException, Depends\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\nrouter = APIRouter()\n\nclass ChatRequest(BaseModel):\n    message: str\n    selected_text: Optional[str] = None\n    conversation_history: Optional[List[Dict]] = []\n\nclass ChatResponse(BaseModel):\n    response: str\n    sources: List[Dict]\n    query: str\n\n@router.post(\"/chat\", response_model=ChatResponse)\nasync def chat_endpoint(request: ChatRequest):\n    try:\n        rag_service = RAGService(\n            qdrant_url=os.getenv(\"QDRANT_URL\"),\n            qdrant_api_key=os.getenv(\"QDRANT_API_KEY\"),\n            openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n        )\n\n        # If selected_text is provided, only search within that content\n        if request.selected_text:\n            # For selected text only, we'll use a different approach\n            # This would involve creating a temporary embedding of the selected text\n            context_chunks = [{\n                'content': request.selected_text,\n                'metadata': {'source': 'user_selected_text'},\n                'score': 1.0\n            }]\n        else:\n            # Normal RAG search\n            context_chunks = rag_service.retrieve_relevant_content(request.message)\n\n        response = rag_service.generate_response(\n            query=request.message,\n            context_chunks=context_chunks,\n            conversation_history=request.conversation_history\n        )\n\n        sources = [chunk['metadata'] for chunk in context_chunks]\n\n        return ChatResponse(\n            response=response,\n            sources=sources,\n            query=request.message\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n"})}),"\n",(0,s.jsx)(n.h3,{id:"frontend-chat-widget-implementation",children:"Frontend Chat Widget Implementation"}),"\n",(0,s.jsx)(n.h4,{id:"react-chat-widget-component",children:"React Chat Widget Component"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-jsx",children:"import React, { useState, useRef, useEffect } from 'react';\n\nconst ChatWidget = ({ selectedText }) => {\n  const [isOpen, setIsOpen] = useState(false);\n  const [messages, setMessages] = useState([]);\n  const [inputValue, setInputValue] = useState('');\n  const [isLoading, setIsLoading] = useState(false);\n  const messagesEndRef = useRef(null);\n\n  const scrollToBottom = () => {\n    messagesEndRef.current?.scrollIntoView({ behavior: \"smooth\" });\n  };\n\n  useEffect(() => {\n    scrollToBottom();\n  }, [messages]);\n\n  const handleSendMessage = async () => {\n    if (!inputValue.trim() || isLoading) return;\n\n    const userMessage = { role: 'user', content: inputValue, timestamp: new Date() };\n    setMessages(prev => [...prev, userMessage]);\n    setInputValue('');\n    setIsLoading(true);\n\n    try {\n      const response = await fetch('/api/chat', {\n        method: 'POST',\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({\n          message: inputValue,\n          selected_text: selectedText,\n          conversation_history: messages.map(msg => ({\n            role: msg.role,\n            content: msg.content\n          }))\n        })\n      });\n\n      const data = await response.json();\n\n      const aiMessage = {\n        role: 'assistant',\n        content: data.response,\n        sources: data.sources,\n        timestamp: new Date()\n      };\n\n      setMessages(prev => [...prev, aiMessage]);\n    } catch (error) {\n      console.error('Error sending message:', error);\n      setMessages(prev => [...prev, {\n        role: 'assistant',\n        content: 'Sorry, I encountered an error. Please try again.',\n        timestamp: new Date()\n      }]);\n    } finally {\n      setIsLoading(false);\n    }\n  };\n\n  const handleKeyDown = (e) => {\n    if (e.key === 'Enter' && !e.shiftKey) {\n      e.preventDefault();\n      handleSendMessage();\n    }\n  };\n\n  return (\n    <div className={`chat-widget ${isOpen ? 'open' : 'closed'}`}>\n      <button\n        className=\"chat-toggle\"\n        onClick={() => setIsOpen(!isOpen)}\n      >\n        \ud83d\udcac AI Assistant\n      </button>\n\n      {isOpen && (\n        <div className=\"chat-container\">\n          <div className=\"chat-header\">\n            <h3>Documentation Assistant</h3>\n            <button\n              className=\"close-button\"\n              onClick={() => setIsOpen(false)}\n            >\n              \xd7\n            </button>\n          </div>\n\n          <div className=\"chat-messages\">\n            {messages.map((msg, index) => (\n              <div key={index} className={`message ${msg.role}`}>\n                <div className=\"message-content\">\n                  {msg.content}\n                  {msg.sources && msg.sources.length > 0 && (\n                    <div className=\"sources\">\n                      <small>Sources: {msg.sources.map(s => s.source).join(', ')}</small>\n                    </div>\n                  )}\n                </div>\n              </div>\n            ))}\n            {isLoading && (\n              <div className=\"message assistant\">\n                <div className=\"message-content\">\n                  <div className=\"typing-indicator\">\n                    <span></span>\n                    <span></span>\n                    <span></span>\n                  </div>\n                </div>\n              </div>\n            )}\n            <div ref={messagesEndRef} />\n          </div>\n\n          <div className=\"chat-input\">\n            <textarea\n              value={inputValue}\n              onChange={(e) => setInputValue(e.target.value)}\n              onKeyDown={handleKeyDown}\n              placeholder=\"Ask about the documentation...\"\n              rows=\"3\"\n            />\n            <button\n              onClick={handleSendMessage}\n              disabled={isLoading || !inputValue.trim()}\n            >\n              Send\n            </button>\n          </div>\n        </div>\n      )}\n    </div>\n  );\n};\n\nexport default ChatWidget;\n"})}),"\n",(0,s.jsx)(n.h3,{id:"advanced-rag-features",children:"Advanced RAG Features"}),"\n",(0,s.jsx)(n.h4,{id:"selected-text-only-mode",children:"Selected Text Only Mode"}),"\n",(0,s.jsx)(n.p,{children:'Implement the "answer only from selected text" functionality:'}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def generate_response_from_selection(\n    self,\n    query: str,\n    selected_text: str,\n    conversation_history: Optional[List[Dict]] = None\n) -> str:\n    """\n    Generate response based only on user-selected text\n    """\n    # Create a system message that restricts the AI to only use the selected text\n    system_message = f"""\n    You are an AI assistant that can only answer questions based on the following text:\n    {selected_text}\n\n    Do not use any other knowledge or information.\n    If the answer cannot be found in the provided text, say so explicitly.\n    """\n\n    messages = [{"role": "system", "content": system_message}]\n\n    if conversation_history:\n        messages.extend(conversation_history)\n\n    messages.append({"role": "user", "content": query})\n\n    response = openai.ChatCompletion.create(\n        model="gpt-3.5-turbo",\n        messages=messages,\n        temperature=0.3,  # Lower temperature for more consistent responses\n        max_tokens=500\n    )\n\n    return response.choices[0].message.content\n'})}),"\n",(0,s.jsx)(n.h4,{id:"context-aware-response-generation",children:"Context-Aware Response Generation"}),"\n",(0,s.jsx)(n.p,{children:"Enhance responses with better context awareness:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'def generate_context_aware_response(\n    self,\n    query: str,\n    context_chunks: List[Dict],\n    conversation_history: Optional[List[Dict]] = None,\n    user_intent: Optional[str] = None\n) -> Dict:\n    """\n    Generate response with enhanced context awareness\n    """\n    # Analyze user intent if not provided\n    if not user_intent:\n        intent_prompt = f"Analyze the user\'s question \'{query}\' and identify the intent. Possible intents: explanation, example, troubleshooting, comparison, definition."\n\n        intent_response = openai.ChatCompletion.create(\n            model="gpt-3.5-turbo",\n            messages=[{"role": "user", "content": intent_prompt}],\n            temperature=0.1\n        )\n        user_intent = intent_response.choices[0].message.content.strip()\n\n    # Build context based on intent\n    context = self._build_intentional_context(context_chunks, user_intent)\n\n    # Generate response with intent-aware system message\n    system_message = f"""\n    You are a documentation assistant helping users understand technical content.\n    The user\'s intent is: {user_intent}\n    Use the following context to provide a helpful response:\n    {context}\n    """\n\n    messages = [{"role": "system", "content": system_message}]\n\n    if conversation_history:\n        messages.extend(conversation_history)\n\n    messages.append({"role": "user", "content": query})\n\n    response = openai.ChatCompletion.create(\n        model="gpt-4",  # Use more capable model for complex queries\n        messages=messages,\n        temperature=0.7,\n        max_tokens=800\n    )\n\n    return {\n        \'response\': response.choices[0].message.content,\n        \'intent\': user_intent,\n        \'confidence\': 0.8  # Placeholder for actual confidence scoring\n    }\n'})}),"\n",(0,s.jsx)(n.h3,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.h4,{id:"caching-strategies",children:"Caching Strategies"}),"\n",(0,s.jsx)(n.p,{children:"Implement caching to improve response times:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import redis\nimport pickle\nfrom functools import wraps\n\nclass RAGCache:\n    def __init__(self, redis_url: str = "redis://localhost:6379"):\n        self.redis_client = redis.from_url(redis_url)\n\n    def get_cached_response(self, query_hash: str) -> Optional[str]:\n        cached = self.redis_client.get(f"rag_response:{query_hash}")\n        return pickle.loads(cached) if cached else None\n\n    def cache_response(self, query_hash: str, response: str, ttl: int = 3600):\n        self.redis_client.setex(\n            f"rag_response:{query_hash}",\n            ttl,\n            pickle.dumps(response)\n        )\n\ndef with_cache(func):\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):\n        query = kwargs.get(\'query\') or args[0] if args else None\n        if query:\n            cache_key = f"rag:{hash(query)}"\n            cached = self.cache.get_cached_response(cache_key)\n            if cached:\n                return cached\n\n        result = func(self, *args, **kwargs)\n\n        if query:\n            self.cache.cache_response(cache_key, result)\n\n        return result\n    return wrapper\n'})}),"\n",(0,s.jsx)(n.h2,{id:"-uiux-angle",children:"\ud83c\udfa8 UI/UX Angle"}),"\n",(0,s.jsx)(n.p,{children:"The chatbot interface should feel natural and helpful, with clear visual indicators for different message types, loading states, and source citations. The interaction should feel conversational while maintaining the professional tone of documentation."}),"\n",(0,s.jsx)(n.h2,{id:"-real-world-example",children:"\ud83d\udcd8 Real-World Example"}),"\n",(0,s.jsx)(n.p,{children:"A RAG chatbot for an API documentation site:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:'User asks: "How do I implement authentication?"'}),"\n",(0,s.jsx)(n.li,{children:"System retrieves relevant sections about authentication methods"}),"\n",(0,s.jsx)(n.li,{children:'AI generates: "To implement authentication, you can use API keys in the header or OAuth tokens..."'}),"\n",(0,s.jsx)(n.li,{children:"Response includes citations to specific documentation sections"}),"\n",(0,s.jsx)(n.li,{children:"User can ask follow-up questions with context maintained"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-common-mistakes--fixes",children:"\u26a0\ufe0f Common Mistakes & Fixes"}),"\n",(0,s.jsx)(n.h3,{id:"mistake-not-handling-token-limits-properly-in-context-assembly",children:"Mistake: Not handling token limits properly in context assembly"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fix"}),": Implement intelligent context truncation that preserves important information."]}),"\n",(0,s.jsx)(n.h3,{id:"mistake-providing-responses-without-source-attribution",children:"Mistake: Providing responses without source attribution"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fix"}),": Always include citations to the documentation sources used."]}),"\n",(0,s.jsx)(n.h3,{id:"mistake-not-differentiating-between-selected-text-and-full-document-responses",children:"Mistake: Not differentiating between selected-text and full-document responses"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Fix"}),": Implement clear visual and functional differences between modes."]}),"\n",(0,s.jsx)(n.h2,{id:"-micro-exercises",children:"\ud83e\uddea Micro-Exercises"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Exercise"}),": Implement a basic RAG service that retrieves and generates responses from sample content."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Exercise"}),": Create a simple chat interface that integrates with your RAG backend."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"-reflection-prompts",children:"\ud83e\udded Reflection Prompts"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"How do users currently find information in your documentation?"}),"\n",(0,s.jsx)(n.li,{children:"What challenges do they face with traditional search?"}),"\n",(0,s.jsx)(n.li,{children:"How might a conversational interface improve their experience?"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"end-of-chapter-summary",children:"End-of-Chapter Summary"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"RAG chatbots combine retrieval and generation for contextual responses"}),"\n",(0,s.jsx)(n.li,{children:"Proper architecture includes frontend, backend, and RAG pipeline components"}),"\n",(0,s.jsx)(n.li,{children:"Selected text functionality requires special handling"}),"\n",(0,s.jsx)(n.li,{children:"Performance optimization is crucial for production use"}),"\n",(0,s.jsx)(n.li,{children:"The system transforms static documentation into interactive assistance"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o});var s=t(6540);const i={},r=s.createContext(i);function o(e){const n=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}}}]);