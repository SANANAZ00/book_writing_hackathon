<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-systems/fastapi-backend" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.0">
<title data-rh="true">FastAPI Backend Setup and Architecture | Physical AI &amp; Humanoid Robotics</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://sananaz00.github.io/book_writing_hackathon/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://sananaz00.github.io/book_writing_hackathon/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://sananaz00.github.io/book_writing_hackathon/docs/systems/fastapi-backend"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="FastAPI Backend Setup and Architecture | Physical AI &amp; Humanoid Robotics"><meta data-rh="true" name="description" content="Hero Insight"><meta data-rh="true" property="og:description" content="Hero Insight"><link data-rh="true" rel="icon" href="/book_writing_hackathon/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://sananaz00.github.io/book_writing_hackathon/docs/systems/fastapi-backend"><link data-rh="true" rel="alternate" href="https://sananaz00.github.io/book_writing_hackathon/docs/systems/fastapi-backend" hreflang="en"><link data-rh="true" rel="alternate" href="https://sananaz00.github.io/book_writing_hackathon/docs/systems/fastapi-backend" hreflang="x-default"><link rel="stylesheet" href="/book_writing_hackathon/assets/css/styles.7785dc77.css">
<script src="/book_writing_hackathon/assets/js/runtime~main.ee4ca24d.js" defer="defer"></script>
<script src="/book_writing_hackathon/assets/js/main.6cf79107.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/book_writing_hackathon/"><div class="navbar__logo"><img src="/book_writing_hackathon/img/logo.svg" alt="Physical AI &amp; Robotics Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/book_writing_hackathon/img/logo.svg" alt="Physical AI &amp; Robotics Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Physical AI &amp; Robotics</b></a><a class="navbar__item navbar__link" href="/book_writing_hackathon/docs/intro">Book</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/SANANAZ00/book_writing_hackathon" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><main class="docMainContainer_TBSr docMainContainerEnhanced_lQrH"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="container container--fluid"><h1 id="fastapi-backend-setup-and-architecture">FastAPI Backend Setup and Architecture</h1>
<h2 id="hero-insight">Hero Insight</h2>
<p>Transform your AI documentation system into a robust, scalable backend service that handles complex RAG operations with efficiency and reliability.</p>
<h2 id="purpose-of-this-chapter">Purpose of This Chapter</h2>
<p>You will build a comprehensive FastAPI backend architecture that supports RAG operations, content generation, user management, and all the services needed for your AI-powered documentation system.</p>
<h2 id="deep-breakdown">Deep Breakdown</h2>
<h3 id="fastapi-backend-architecture-overview">FastAPI Backend Architecture Overview</h3>
<p>A production-ready FastAPI backend for AI documentation includes multiple layers:</p>
<h4 id="1-api-layer">1. API Layer</h4>
<ul>
<li><strong>Route Organization</strong>: Well-structured endpoints for different functionalities</li>
<li><strong>Request Validation</strong>: Pydantic models for data validation</li>
<li><strong>Response Serialization</strong>: Consistent response formats</li>
<li><strong>Error Handling</strong>: Proper HTTP status codes and error messages</li>
</ul>
<h4 id="2-service-layer">2. Service Layer</h4>
<ul>
<li><strong>Business Logic</strong>: Core functionality implementations</li>
<li><strong>External API Integration</strong>: OpenAI, Qdrant, and other services</li>
<li><strong>Caching</strong>: Performance optimization strategies</li>
<li><strong>Rate Limiting</strong>: API usage control</li>
</ul>
<h4 id="3-data-layer">3. Data Layer</h4>
<ul>
<li><strong>Model Definitions</strong>: Pydantic models for data structures</li>
<li><strong>Database Abstractions</strong>: Qdrant client integration</li>
<li><strong>Data Validation</strong>: Consistent data integrity</li>
<li><strong>Transformation Logic</strong>: Data processing and formatting</li>
</ul>
<h4 id="4-utility-layer">4. Utility Layer</h4>
<ul>
<li><strong>Configuration Management</strong>: Environment-based settings</li>
<li><strong>Logging</strong>: Comprehensive system monitoring</li>
<li><strong>Security</strong>: Authentication and authorization</li>
<li><strong>Health Checks</strong>: System status monitoring</li>
</ul>
<h3 id="project-structure">Project Structure</h3>
<pre><code>backend/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI application instance
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ database.py         # Database connections
‚îÇ   ‚îú‚îÄ‚îÄ models/             # Pydantic models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ request.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ response.py
‚îÇ   ‚îú‚îÄ‚îÄ schemas/            # Pydantic schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ content.py
‚îÇ   ‚îú‚îÄ‚îÄ services/           # Business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag_service.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ content_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ embedding_service.py
‚îÇ   ‚îú‚îÄ‚îÄ routes/             # API endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chat.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ rag.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ content.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/              # Utility functions
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ logging.py
‚îÇ       ‚îú‚îÄ‚îÄ validation.py
‚îÇ       ‚îî‚îÄ‚îÄ security.py
‚îú‚îÄ‚îÄ tests/                  # Test files
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ .env
</code></pre>
<h3 id="core-fastapi-application-setup">Core FastAPI Application Setup</h3>
<h4 id="main-application-file">Main Application File</h4>
<p><strong>backend/app/main.py:</strong></p>
<pre><code class="language-python">from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import logging
import time

from app.config import settings
from app.routes import chat, rag, content
from app.utils.logging import setup_logging
from app.database import init_db

@asynccontextmanager
async def lifespan(app: FastAPI):
    &quot;&quot;&quot;
    Application lifespan events
    &quot;&quot;&quot;
    # Startup
    setup_logging()
    await init_db()
    logging.info(&quot;Application started&quot;)
    yield
    # Shutdown
    logging.info(&quot;Application shutting down&quot;)

app = FastAPI(
    title=&quot;AI Documentation Backend&quot;,
    description=&quot;Backend services for AI-powered documentation system&quot;,
    version=&quot;1.0.0&quot;,
    lifespan=lifespan
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=[&quot;*&quot;],
    allow_headers=[&quot;*&quot;],
)

# Include routers
app.include_router(chat.router, prefix=&quot;/api/chat&quot;, tags=[&quot;chat&quot;])
app.include_router(rag.router, prefix=&quot;/api/rag&quot;, tags=[&quot;rag&quot;])
app.include_router(content.router, prefix=&quot;/api/content&quot;, tags=[&quot;content&quot;])

@app.middleware(&quot;http&quot;)
async def add_process_time_header(request: Request, call_next):
    &quot;&quot;&quot;
    Add processing time to response headers
    &quot;&quot;&quot;
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers[&quot;X-Process-Time&quot;] = str(process_time)
    return response

@app.get(&quot;/&quot;)
async def root():
    return {&quot;message&quot;: &quot;AI Documentation Backend API&quot;, &quot;version&quot;: &quot;1.0.0&quot;}

@app.get(&quot;/health&quot;)
async def health_check():
    return {&quot;status&quot;: &quot;healthy&quot;, &quot;timestamp&quot;: time.time()}

@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    &quot;&quot;&quot;
    Global exception handler
    &quot;&quot;&quot;
    logging.error(f&quot;Unhandled exception: {str(exc)}&quot;, exc_info=True)
    return JSONResponse(
        status_code=500,
        content={&quot;detail&quot;: &quot;Internal server error&quot;}
    )
</code></pre>
<h4 id="configuration-management">Configuration Management</h4>
<p><strong>backend/app/config.py:</strong></p>
<pre><code class="language-python">from pydantic_settings import BaseSettings
from typing import List, Optional

class Settings(BaseSettings):
    # API Configuration
    API_TITLE: str = &quot;AI Documentation Backend&quot;
    API_VERSION: str = &quot;1.0.0&quot;

    # Environment
    ENVIRONMENT: str = &quot;development&quot;
    DEBUG: bool = False

    # OpenAI Configuration
    OPENAI_API_KEY: str
    OPENAI_MODEL: str = &quot;gpt-3.5-turbo&quot;
    EMBEDDING_MODEL: str = &quot;text-embedding-ada-002&quot;

    # Qdrant Configuration
    QDRANT_URL: str
    QDRANT_API_KEY: str
    QDRANT_COLLECTION_NAME: str = &quot;documentation&quot;

    # Database Configuration
    QDRANT_GRPC_ENABLED: bool = True
    QDRANT_TIMEOUT: int = 30

    # Application Settings
    ALLOWED_ORIGINS: List[str] = [&quot;http://localhost:3000&quot;, &quot;http://localhost:8080&quot;]
    CORS_ALLOW_CREDENTIALS: bool = True
    CORS_ALLOW_METHODS: List[str] = [&quot;*&quot;]
    CORS_ALLOW_HEADERS: List[str] = [&quot;*&quot;]

    # Performance Settings
    RAG_SEARCH_LIMIT: int = 5
    RAG_SCORE_THRESHOLD: float = 0.3
    MAX_TOKENS: int = 500
    TEMPERATURE: float = 0.7

    # Rate Limiting
    RATE_LIMIT_REQUESTS: int = 100
    RATE_LIMIT_WINDOW: int = 60  # seconds

    class Config:
        env_file = &quot;.env&quot;
        case_sensitive = True

settings = Settings()
</code></pre>
<h3 id="data-models-and-schemas">Data Models and Schemas</h3>
<h4 id="request-and-response-models">Request and Response Models</h4>
<p><strong>backend/app/schemas/chat.py:</strong></p>
<pre><code class="language-python">from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from datetime import datetime

class ChatMessage(BaseModel):
    role: str  # &quot;user&quot; or &quot;assistant&quot;
    content: str
    timestamp: Optional[datetime] = None

class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=2000)
    selected_text: Optional[str] = None
    conversation_history: List[ChatMessage] = []
    context_only: bool = False
    temperature: float = Field(default=0.7, ge=0.0, le=1.0)
    max_tokens: int = Field(default=500, ge=100, le=1000)

class ChatResponse(BaseModel):
    response: str
    sources: List[Dict[str, Any]]
    query: str
    conversation_id: Optional[str] = None
    tokens_used: int = 0
    processing_time: float = 0.0

class ChatStreamChunk(BaseModel):
    content: str
    type: str = &quot;chunk&quot;  # &quot;start&quot;, &quot;chunk&quot;, &quot;end&quot;
    metadata: Optional[Dict[str, Any]] = None
</code></pre>
<p><strong>backend/app/schemas/rag.py:</strong></p>
<pre><code class="language-python">from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional

class SearchRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=1000)
    limit: int = Field(default=5, ge=1, le=20)
    score_threshold: float = Field(default=0.3, ge=0.0, le=1.0)
    filters: Optional[Dict[str, Any]] = None

class SearchResult(BaseModel):
    id: str
    content: str
    metadata: Dict[str, Any]
    score: float

class SearchResponse(BaseModel):
    results: List[SearchResult]
    query: str
    processing_time: float

class RAGRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=1000)
    context_only: bool = False
    selected_text: Optional[str] = None
    max_results: int = Field(default=5, ge=1, le=20)
    temperature: float = Field(default=0.7, ge=0.0, le=1.0)

class RAGResponse(BaseModel):
    answer: str
    sources: List[Dict[str, Any]]
    query: str
    confidence_score: float
    processing_time: float
</code></pre>
<h3 id="service-layer-implementation">Service Layer Implementation</h3>
<h4 id="rag-service">RAG Service</h4>
<p><strong>backend/app/services/rag_service.py:</strong></p>
<pre><code class="language-python">import logging
from typing import List, Dict, Any, Optional, Tuple
from qdrant_client import QdrantClient
from qdrant_client.http import models
import openai
import tiktoken
from app.config import settings

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        self.qdrant_client = QdrantClient(
            url=settings.QDRANT_URL,
            api_key=settings.QDRANT_API_KEY,
            prefer_grpc=settings.QDRANT_GRPC_ENABLED
        )
        openai.api_key = settings.OPENAI_API_KEY
        self.encoder = tiktoken.encoding_for_model(settings.OPENAI_MODEL)

    async def search_content(
        self,
        query: str,
        limit: int = 5,
        score_threshold: float = 0.3,
        filters: Optional[Dict[str, Any]] = None
    ) -&gt; List[Dict[str, Any]]:
        &quot;&quot;&quot;
        Search for relevant content in the vector database
        &quot;&quot;&quot;
        try:
            # Generate embedding for query
            response = openai.Embedding.create(
                input=query,
                model=settings.EMBEDDING_MODEL
            )
            query_embedding = response[&#x27;data&#x27;][0][&#x27;embedding&#x27;]

            # Build search filters
            search_filter = None
            if filters:
                filter_conditions = []
                for key, value in filters.items():
                    filter_conditions.append(
                        models.FieldCondition(
                            key=key,
                            match=models.MatchValue(value=value)
                        )
                    )
                if filter_conditions:
                    search_filter = models.Filter(
                        must=filter_conditions
                    )

            # Perform search in Qdrant
            search_results = self.qdrant_client.search(
                collection_name=settings.QDRANT_COLLECTION_NAME,
                query_vector=query_embedding,
                limit=limit,
                score_threshold=score_threshold,
                with_payload=True,
                query_filter=search_filter
            )

            return [
                {
                    &#x27;id&#x27;: str(result.id),
                    &#x27;content&#x27;: result.payload.get(&#x27;content&#x27;, &#x27;&#x27;),
                    &#x27;metadata&#x27;: result.payload.get(&#x27;metadata&#x27;, {}),
                    &#x27;score&#x27;: result.score
                }
                for result in search_results
            ]

        except Exception as e:
            logger.error(f&quot;Error in search_content: {str(e)}&quot;)
            raise

    async def generate_response(
        self,
        query: str,
        context_chunks: List[Dict[str, Any]],
        conversation_history: Optional[List[Dict[str, str]]] = None,
        selected_text: Optional[str] = None,
        temperature: float = 0.7
    ) -&gt; Tuple[str, Dict[str, Any]]:
        &quot;&quot;&quot;
        Generate response using context and conversation history
        &quot;&quot;&quot;
        try:
            # Build context based on mode
            if selected_text:
                # Mode: Answer only from selected text
                context = f&quot;Answer only based on this text: {selected_text}&quot;
            else:
                # Mode: Use retrieved context
                context = self._assemble_context(context_chunks)

            # Build system message
            system_message = f&quot;&quot;&quot;
            You are an AI documentation assistant.
            Use the following context to answer the user&#x27;s question: {context}

            Provide helpful, accurate responses based on the provided information.
            If the answer cannot be found in the provided context, acknowledge this.
            &quot;&quot;&quot;

            # Build conversation messages
            messages = [{&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_message}]

            if conversation_history:
                messages.extend(conversation_history)

            messages.append({&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: query})

            # Generate response
            response = openai.ChatCompletion.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                temperature=temperature,
                max_tokens=settings.MAX_TOKENS
            )

            generated_text = response.choices[0].message.content
            tokens_used = response.usage.total_tokens if response.usage else 0

            return generated_text, {
                &#x27;tokens_used&#x27;: tokens_used,
                &#x27;model_used&#x27;: settings.OPENAI_MODEL,
                &#x27;confidence&#x27;: 0.8  # Placeholder for actual confidence calculation
            }

        except Exception as e:
            logger.error(f&quot;Error in generate_response: {str(e)}&quot;)
            raise

    def _assemble_context(self, chunks: List[Dict[str, Any]]) -&gt; str:
        &quot;&quot;&quot;
        Assemble context from retrieved chunks, respecting token limits
        &quot;&quot;&quot;
        context_parts = []
        total_tokens = 0
        max_tokens = 3000  # Leave room for conversation and response

        for chunk in chunks:
            chunk_text = f&quot;Source: {chunk.get(&#x27;metadata&#x27;, {}).get(&#x27;source&#x27;, &#x27;Unknown&#x27;)}\nContent: {chunk[&#x27;content&#x27;]}\n\n&quot;
            chunk_tokens = len(self.encoder.encode(chunk_text))

            if total_tokens + chunk_tokens &gt; max_tokens:
                break

            context_parts.append(chunk_text)
            total_tokens += chunk_tokens

        return &quot;&quot;.join(context_parts)

    async def process_rag_query(
        self,
        query: str,
        limit: int = 5,
        score_threshold: float = 0.3,
        selected_text: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ) -&gt; Tuple[str, List[Dict[str, Any]], Dict[str, Any]]:
        &quot;&quot;&quot;
        Complete RAG processing: search + generate response
        &quot;&quot;&quot;
        import time
        start_time = time.time()

        # Search for relevant content (unless using selected text only)
        if not selected_text:
            context_chunks = await self.search_content(
                query=query,
                limit=limit,
                score_threshold=score_threshold
            )
        else:
            # For selected text mode, create a single chunk
            context_chunks = [{
                &#x27;id&#x27;: &#x27;selected_text&#x27;,
                &#x27;content&#x27;: selected_text,
                &#x27;metadata&#x27;: {&#x27;source&#x27;: &#x27;user_selected_text&#x27;},
                &#x27;score&#x27;: 1.0
            }]

        # Generate response
        response, metadata = await self.generate_response(
            query=query,
            context_chunks=context_chunks,
            conversation_history=conversation_history,
            selected_text=selected_text
        )

        processing_time = time.time() - start_time

        return response, context_chunks, {
            **metadata,
            &#x27;processing_time&#x27;: processing_time
        }
</code></pre>
<h4 id="content-service">Content Service</h4>
<p><strong>backend/app/services/content_service.py:</strong></p>
<pre><code class="language-python">import logging
from typing import List, Dict, Any, Optional
from app.config import settings
import openai

logger = logging.getLogger(__name__)

class ContentService:
    def __init__(self):
        openai.api_key = settings.OPENAI_API_KEY

    async def generate_content(
        self,
        topic: str,
        style: str = &quot;educational&quot;,
        length: str = &quot;medium&quot;,
        target_audience: str = &quot;intermediate&quot;,
        context: Optional[str] = None
    ) -&gt; Dict[str, Any]:
        &quot;&quot;&quot;
        Generate educational content using AI
        &quot;&quot;&quot;
        try:
            prompt = self._build_content_prompt(
                topic=topic,
                style=style,
                length=length,
                target_audience=target_audience,
                context=context
            )

            response = openai.ChatCompletion.create(
                model=settings.OPENAI_MODEL,
                messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}],
                temperature=0.7,
                max_tokens=1000
            )

            content = response.choices[0].message.content

            return {
                &#x27;content&#x27;: content,
                &#x27;metadata&#x27;: {
                    &#x27;topic&#x27;: topic,
                    &#x27;style&#x27;: style,
                    &#x27;target_audience&#x27;: target_audience,
                    &#x27;tokens_used&#x27;: response.usage.total_tokens if response.usage else 0
                }
            }

        except Exception as e:
            logger.error(f&quot;Error in generate_content: {str(e)}&quot;)
            raise

    def _build_content_prompt(
        self,
        topic: str,
        style: str,
        length: str,
        target_audience: str,
        context: Optional[str] = None
    ) -&gt; str:
        &quot;&quot;&quot;
        Build prompt for content generation
        &quot;&quot;&quot;
        base_prompt = f&quot;&quot;&quot;
        Write educational content about &quot;{topic}&quot; for {target_audience} level learners.
        Write in a {style} style and make it {length} length.

        Include these sections:
        1. Hero Insight: [1-2 line emotional opening]
        2. Purpose: [Clear statement of learning objective]
        3. Deep Breakdown: [Main content with clear subsections]
        4. UI/UX Angle: [Visual translation when needed]
        5. Real-World Example: [Short, high-quality case]
        6. Common Mistakes &amp; Fixes: [Practical troubleshooting]
        7. Micro-Exercises: [Small actionable tasks]
        8. End-of-Section Summary: [5-7 bullet takeaways]
        &quot;&quot;&quot;

        if context:
            base_prompt = f&quot;{base_prompt}\n\nContext: {context}&quot;

        return base_prompt
</code></pre>
<h3 id="api-routes-implementation">API Routes Implementation</h3>
<h4 id="chat-routes">Chat Routes</h4>
<p><strong>backend/app/routes/chat.py:</strong></p>
<pre><code class="language-python">from fastapi import APIRouter, Depends, HTTPException
from typing import List
import time
import logging
from app.schemas.chat import ChatRequest, ChatResponse, ChatStreamChunk
from app.schemas.rag import RAGRequest, RAGResponse
from app.services.rag_service import RAGService
from app.config import settings

router = APIRouter()
logger = logging.getLogger(__name__)

@router.post(&quot;/&quot;, response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    &quot;&quot;&quot;
    Main chat endpoint that handles user queries
    &quot;&quot;&quot;
    start_time = time.time()

    try:
        rag_service = RAGService()

        response, sources, metadata = await rag_service.process_rag_query(
            query=request.message,
            selected_text=request.selected_text,
            conversation_history=[
                {&#x27;role&#x27;: msg.role, &#x27;content&#x27;: msg.content}
                for msg in request.conversation_history
            ]
        )

        processing_time = time.time() - start_time

        return ChatResponse(
            response=response,
            sources=sources,
            query=request.message,
            tokens_used=metadata.get(&#x27;tokens_used&#x27;, 0),
            processing_time=processing_time
        )

    except Exception as e:
        logger.error(f&quot;Chat endpoint error: {str(e)}&quot;)
        raise HTTPException(status_code=500, detail=str(e))

@router.get(&quot;/stream&quot;)
async def chat_stream_endpoint():
    &quot;&quot;&quot;
    Streaming chat endpoint for real-time responses
    &quot;&quot;&quot;
    # Implementation for streaming responses would go here
    # This would use Server-Sent Events (SSE) or WebSockets
    pass
</code></pre>
<h4 id="rag-routes">RAG Routes</h4>
<p><strong>backend/app/routes/rag.py:</strong></p>
<pre><code class="language-python">from fastapi import APIRouter, HTTPException
import time
import logging
from app.schemas.rag import SearchRequest, SearchResponse, RAGRequest, RAGResponse
from app.services.rag_service import RAGService

router = APIRouter()
logger = logging.getLogger(__name__)

@router.post(&quot;/search&quot;, response_model=SearchResponse)
async def search_endpoint(request: SearchRequest):
    &quot;&quot;&quot;
    Semantic search endpoint
    &quot;&quot;&quot;
    start_time = time.time()

    try:
        rag_service = RAGService()

        results = await rag_service.search_content(
            query=request.query,
            limit=request.limit,
            score_threshold=request.score_threshold,
            filters=request.filters
        )

        processing_time = time.time() - start_time

        return SearchResponse(
            results=results,
            query=request.query,
            processing_time=processing_time
        )

    except Exception as e:
        logger.error(f&quot;Search endpoint error: {str(e)}&quot;)
        raise HTTPException(status_code=500, detail=str(e))

@router.post(&quot;/answer&quot;, response_model=RAGResponse)
async def rag_answer_endpoint(request: RAGRequest):
    &quot;&quot;&quot;
    Complete RAG answer generation endpoint
    &quot;&quot;&quot;
    start_time = time.time()

    try:
        rag_service = RAGService()

        response, sources, metadata = await rag_service.process_rag_query(
            query=request.query,
            limit=request.max_results,
            selected_text=request.selected_text
        )

        processing_time = time.time() - start_time

        return RAGResponse(
            answer=response,
            sources=[result[&#x27;metadata&#x27;] for result in sources],
            query=request.query,
            confidence_score=metadata.get(&#x27;confidence&#x27;, 0.8),
            processing_time=processing_time
        )

    except Exception as e:
        logger.error(f&quot;RAG answer endpoint error: {str(e)}&quot;)
        raise HTTPException(status_code=500, detail=str(e))
</code></pre>
<h3 id="performance-and-optimization">Performance and Optimization</h3>
<h4 id="caching-implementation">Caching Implementation</h4>
<pre><code class="language-python">import redis
import hashlib
import pickle
from functools import wraps

class CacheService:
    def __init__(self, redis_url: str = &quot;redis://localhost:6379&quot;):
        self.redis_client = redis.from_url(redis_url)

    def get_cache_key(self, prefix: str, *args, **kwargs) -&gt; str:
        &quot;&quot;&quot;Generate cache key from function arguments&quot;&quot;&quot;
        key_data = f&quot;{prefix}:{str(args)}:{str(kwargs)}&quot;
        return f&quot;cache:{hashlib.md5(key_data.encode()).hexdigest()}&quot;

    def cached(self, ttl: int = 300):
        &quot;&quot;&quot;Decorator for caching function results&quot;&quot;&quot;
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                cache_key = self.get_cache_key(func.__name__, *args, **kwargs)

                # Try to get from cache
                cached_result = self.redis_client.get(cache_key)
                if cached_result:
                    return pickle.loads(cached_result)

                # Execute function
                result = await func(*args, **kwargs)

                # Cache result
                self.redis_client.setex(
                    cache_key,
                    ttl,
                    pickle.dumps(result)
                )

                return result
            return wrapper
        return decorator

# Usage in services
cache_service = CacheService()

class RAGService:
    # ... existing code ...

    @cache_service.cached(ttl=600)  # Cache for 10 minutes
    async def search_content(self, query: str, limit: int = 5, score_threshold: float = 0.3):
        # ... existing search logic ...
        pass
</code></pre>
<h4 id="rate-limiting">Rate Limiting</h4>
<pre><code class="language-python">from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from fastapi import Request

limiter = Limiter(key_func=get_remote_address)

app.state.limiter = limiter
app.add_exception_handler(429, _rate_limit_exceeded_handler)

@router.post(&quot;/&quot;, response_model=ChatResponse)
@limiter.limit(&quot;10/minute&quot;)  # 10 requests per minute per IP
async def chat_endpoint(request: ChatRequest, request_obj: Request):
    # ... existing implementation ...
    pass
</code></pre>
<h2 id="-uiux-angle">üé® UI/UX Angle</h2>
<p>The backend performance directly impacts the user experience. Fast response times, proper error handling, and reliable service availability are crucial for maintaining a positive user experience with the AI assistant.</p>
<h2 id="-real-world-example">üìò Real-World Example</h2>
<p>A production FastAPI backend handles:</p>
<ol>
<li>High-volume chat requests during peak usage</li>
<li>Complex RAG operations across large documentation sets</li>
<li>Proper error handling and fallback mechanisms</li>
<li>Performance monitoring and optimization</li>
<li>Secure API key management and rate limiting</li>
</ol>
<h2 id="Ô∏è-common-mistakes--fixes">‚ö†Ô∏è Common Mistakes &amp; Fixes</h2>
<h3 id="mistake-not-implementing-proper-error-handling-and-logging">Mistake: Not implementing proper error handling and logging</h3>
<p><strong>Fix</strong>: Add comprehensive exception handling and structured logging throughout the application.</p>
<h3 id="mistake-hardcoding-configuration-values-instead-of-using-environment-variables">Mistake: Hardcoding configuration values instead of using environment variables</h3>
<p><strong>Fix</strong>: Use Pydantic Settings for all configuration management.</p>
<h3 id="mistake-not-validating-input-data-properly">Mistake: Not validating input data properly</h3>
<p><strong>Fix</strong>: Use Pydantic models for comprehensive request validation.</p>
<h2 id="-micro-exercises">üß™ Micro-Exercises</h2>
<ol>
<li><strong>Exercise</strong>: Set up the FastAPI project structure with proper routing and models.</li>
<li><strong>Exercise</strong>: Implement a basic RAG service with search and response generation.</li>
</ol>
<h2 id="-reflection-prompts">üß≠ Reflection Prompts</h2>
<ul>
<li>How does your current backend architecture handle AI service integration?</li>
<li>What challenges have you faced with API performance and reliability?</li>
<li>How might proper backend architecture improve your AI application?</li>
</ul>
<h2 id="end-of-chapter-summary">End-of-Chapter Summary</h2>
<ul>
<li>FastAPI provides a robust foundation for AI backend services</li>
<li>Proper architecture includes multiple service layers</li>
<li>Configuration management is crucial for deployment flexibility</li>
<li>Performance optimization ensures good user experience</li>
<li>The backend architecture enables scalable AI documentation systems</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/SANANAZ00/book_writing_hackathon/edit/main/docs/systems/fastapi-backend.mdx" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#hero-insight" class="table-of-contents__link toc-highlight">Hero Insight</a></li><li><a href="#purpose-of-this-chapter" class="table-of-contents__link toc-highlight">Purpose of This Chapter</a></li><li><a href="#deep-breakdown" class="table-of-contents__link toc-highlight">Deep Breakdown</a><ul><li><a href="#fastapi-backend-architecture-overview" class="table-of-contents__link toc-highlight">FastAPI Backend Architecture Overview</a></li><li><a href="#project-structure" class="table-of-contents__link toc-highlight">Project Structure</a></li><li><a href="#core-fastapi-application-setup" class="table-of-contents__link toc-highlight">Core FastAPI Application Setup</a></li><li><a href="#data-models-and-schemas" class="table-of-contents__link toc-highlight">Data Models and Schemas</a></li><li><a href="#service-layer-implementation" class="table-of-contents__link toc-highlight">Service Layer Implementation</a></li><li><a href="#api-routes-implementation" class="table-of-contents__link toc-highlight">API Routes Implementation</a></li><li><a href="#performance-and-optimization" class="table-of-contents__link toc-highlight">Performance and Optimization</a></li></ul></li><li><a href="#-uiux-angle" class="table-of-contents__link toc-highlight">üé® UI/UX Angle</a></li><li><a href="#-real-world-example" class="table-of-contents__link toc-highlight">üìò Real-World Example</a></li><li><a href="#Ô∏è-common-mistakes--fixes" class="table-of-contents__link toc-highlight">‚ö†Ô∏è Common Mistakes &amp; Fixes</a><ul><li><a href="#mistake-not-implementing-proper-error-handling-and-logging" class="table-of-contents__link toc-highlight">Mistake: Not implementing proper error handling and logging</a></li><li><a href="#mistake-hardcoding-configuration-values-instead-of-using-environment-variables" class="table-of-contents__link toc-highlight">Mistake: Hardcoding configuration values instead of using environment variables</a></li><li><a href="#mistake-not-validating-input-data-properly" class="table-of-contents__link toc-highlight">Mistake: Not validating input data properly</a></li></ul></li><li><a href="#-micro-exercises" class="table-of-contents__link toc-highlight">üß™ Micro-Exercises</a></li><li><a href="#-reflection-prompts" class="table-of-contents__link toc-highlight">üß≠ Reflection Prompts</a></li><li><a href="#end-of-chapter-summary" class="table-of-contents__link toc-highlight">End-of-Chapter Summary</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Docs</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/book_writing_hackathon/docs/intro">Introduction</a></li></ul></div><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://stackoverflow.com/tagged/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Stack Overflow<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://discordapp.com/invite/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/docusaurus" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/SANANAZ00/book_writing_hackathon" target="_blank" rel="noopener noreferrer" class="footer__link-item">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2025 Physical AI & Humanoid Robotics. Built with Docusaurus.</div></div></div></footer><div class="chatbotWidget_q_OK"><button class="chatToggle_zvSz" aria-label="Open chat">üí¨ AI Assistant</button></div></div>
</body>
</html>